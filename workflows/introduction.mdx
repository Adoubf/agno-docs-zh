---
title: ä»€ä¹ˆæ˜¯å·¥ä½œæµï¼Ÿ
sidebarTitle: æ¦‚è§ˆ
---
å·¥ä½œæµæ˜¯ä¸“ä¸ºç”Ÿäº§åº”ç”¨è®¾è®¡çš„ç¡®å®šæ€§ã€æœ‰çŠ¶æ€ã€å¤šæ™ºèƒ½ä½“ç¨‹åºã€‚å®ƒä»¬ç»è¿‡å®æˆ˜æ£€éªŒï¼ŒåŠŸèƒ½æå…¶å¼ºå¤§ï¼Œå¹¶æä¾›ä»¥ä¸‹ä¼˜åŠ¿ï¼š

- **çº¯Python**ï¼šä½¿ç”¨æ ‡å‡†Pythonæ„å»ºå·¥ä½œæµé€»è¾‘ã€‚åœ¨å¼€å‘äº†æ•°ç™¾ä¸ªæ™ºèƒ½ä½“ç³»ç»Ÿåï¼Œ**æ²¡æœ‰ä»»ä½•æ¡†æ¶æˆ–åŸºäºæ­¥éª¤çš„æ–¹æ³•èƒ½æ¯”çº¯Pythonæä¾›æ›´é«˜çš„çµæ´»æ€§å’Œå¯é æ€§**ã€‚æƒ³è¦å¾ªç¯ï¼Ÿä½¿ç”¨while/forï¼›æƒ³è¦æ¡ä»¶åˆ¤æ–­ï¼Ÿä½¿ç”¨if/elseï¼›æƒ³è¦å¼‚å¸¸å¤„ç†ï¼Ÿä½¿ç”¨try/exceptã€‚
- **å®Œå…¨æ§åˆ¶ä¸çµæ´»æ€§**ï¼šç”±äºå·¥ä½œæµé€»è¾‘æ˜¯Pythonå‡½æ•°ï¼Œæ‚¨å¯ä»¥å®Œå…¨æŒæ§æ•´ä¸ªæµç¨‹ï¼Œä¾‹å¦‚åœ¨å¤„ç†å‰éªŒè¯è¾“å…¥ã€å¹¶è¡Œç”Ÿæˆå’Œè¿è¡Œæ™ºèƒ½ä½“ã€æŒ‰éœ€ç¼“å­˜ç»“æœä»¥åŠçº æ­£ä¸­é—´é”™è¯¯ã€‚**è¿™ç§æ§åˆ¶çº§åˆ«å¯¹å¯é æ€§è‡³å…³é‡è¦ã€‚**
- **å†…ç½®å­˜å‚¨ä¸ç¼“å­˜**ï¼šå·¥ä½œæµè‡ªå¸¦å­˜å‚¨å’ŒçŠ¶æ€ç®¡ç†åŠŸèƒ½ã€‚ä½¿ç”¨session_stateç¼“å­˜ä¸­é—´ç»“æœã€‚è¿™ç§æ–¹æ³•çš„ä¸€å¤§ä¼˜åŠ¿æ˜¯ï¼Œæ‚¨å¯ä»¥åœ¨å•ç‹¬è¿›ç¨‹ä¸­è§¦å‘å·¥ä½œæµå¹¶ç¨åæŸ¥è¯¢ç»“æœï¼Œä»è€Œé¿å…é•¿æ—¶é—´è¿è¡Œå·¥ä½œæµå¸¸è§çš„è¯·æ±‚è¶…æ—¶é—®é¢˜ã€‚

<Check>

ç”±äºå·¥ä½œæµé€»è¾‘æ˜¯Pythonå‡½æ•°ï¼ŒAIä»£ç ç¼–è¾‘å™¨å¯ä»¥ä¸ºæ‚¨ç¼–å†™å·¥ä½œæµã€‚åªéœ€æ·»åŠ `https://docs.agno.com`ä½œä¸ºæ–‡æ¡£æºã€‚

</Check>

### æœ€å¤§äº®ç‚¹

æ— éœ€å­¦ä¹ æ–°çŸ¥è¯†ï¼æ‚¨å·²ç»æŒæ¡Pythonï¼Œå·²ç»çŸ¥é“å¦‚ä½•æ„å»ºæ™ºèƒ½ä½“å’Œå›¢é˜Ÿâ€”â€”ç°åœ¨åªéœ€ç”¨å¸¸è§„Pythonä»£ç å°†å®ƒä»¬ç»„åˆèµ·æ¥ã€‚ä¸å¿…å­¦ä¹ æ–°çš„DSLæˆ–è¯­æ³•ã€‚

è¿™æ˜¯ä¸€ä¸ªç¼“å­˜è¾“å‡ºçš„ç®€å•å·¥ä½œæµç¤ºä¾‹ã€‚æ‚¨å¯ä»¥çœ‹åˆ°å¯¹æµç¨‹çš„æ§åˆ¶çº§åˆ«ï¼Œç”šè‡³"å­˜å‚¨çŠ¶æ€"éƒ½æ˜¯åœ¨å“åº”ç”Ÿæˆåè¿›è¡Œçš„ã€‚

```python simple_cache_workflow.py
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class CacheWorkflow(Workflow):
    # çº¯æè¿°æ€§å†…å®¹ï¼Œä¸ç”¨äºå·¥ä½œæµç¨‹
    description: str = "A workflow that caches previous outputs"

    # åœ¨å·¥ä½œæµä¸­å°†æ™ºèƒ½ä½“æˆ–å›¢é˜Ÿæ·»åŠ ä¸ºå±æ€§
    agent = Agent(model=OpenAIChat(id="gpt-4o-mini"))

    # åœ¨ `run()` æ–¹æ³•ä¸­ç¼–å†™é€»è¾‘
    def run(self, message: str) -> Iterator[RunResponse]:
        logger.info(f"Checking cache for '{message}'")
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦å·²ç¼“å­˜
        if self.session_state.get(message):
            logger.info(f"Cache hit for '{message}'")
            yield RunResponse(run_id=self.run_id, content=self.session_state.get(message))
            return

        logger.info(f"Cache miss for '{message}'")
        # è¿è¡Œæ™ºèƒ½ä½“å¹¶è¿”å›å“åº”
        yield from self.agent.run(message, stream=True)

        # åœ¨å“åº”ç”Ÿæˆåç¼“å­˜è¾“å‡º
        self.session_state[message] = self.agent.run_response.content


if __name__ == "__main__":
    workflow = CacheWorkflow()
    # è¿è¡Œå·¥ä½œæµï¼ˆè€—æ—¶çº¦1ç§’ï¼‰
    response: Iterator[RunResponse] = workflow.run(message="Tell me a joke.")
    # è¾“å‡ºå“åº”
    pprint_run_response(response, markdown=True, show_time=True)
    # å†æ¬¡è¿è¡Œå·¥ä½œæµç¨‹ï¼ˆç”±äºç¼“å­˜æœºåˆ¶ä¼šç«‹å³æ‰§è¡Œï¼‰
    response: Iterator[RunResponse] = workflow.run(message="Tell me a joke.")
    # è¾“å‡ºå“åº”å†…å®¹
    pprint_run_response(response, markdown=True, show_time=True)
```

### å¦‚ä½•æ„å»ºå·¥ä½œæµ

1. é€šè¿‡ç»§æ‰¿`Workflow`ç±»æ¥å®šä¹‰æ‚¨çš„å·¥ä½œæµç±»ã€‚
2. å°†æ™ºèƒ½ä½“æˆ–å›¢é˜Ÿæ·»åŠ ä¸ºå·¥ä½œæµçš„å±æ€§ã€‚è¿™ä¸æ˜¯ä¸¥æ ¼è¦æ±‚çš„ï¼Œä½†æœ‰åŠ©äºæˆ‘ä»¬å°†æ™ºèƒ½ä½“çš„session_idæ˜ å°„åˆ°å·¥ä½œæµçš„session_idã€‚
3. åœ¨`run()`æ–¹æ³•ä¸­å®ç°å·¥ä½œæµé€»è¾‘ã€‚è¿™æ˜¯è¿è¡Œå·¥ä½œæµæ—¶è°ƒç”¨çš„ä¸»è¦å‡½æ•°ï¼ˆ**å·¥ä½œæµå…¥å£ç‚¹**ï¼‰ã€‚è¯¥å‡½æ•°èµ‹äºˆæˆ‘ä»¬å¯¹æµç¨‹çš„æå¤§æ§åˆ¶æƒï¼šéƒ¨åˆ†æ™ºèƒ½ä½“å¯ä»¥æµå¼ä¼ è¾“ï¼Œå…¶ä»–å¯ä»¥ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºï¼Œæ™ºèƒ½ä½“å¯ä»¥ä½¿ç”¨`async.gather()`å¹¶è¡Œè¿è¡Œï¼ŒæŸäº›æ™ºèƒ½ä½“å¯ä»¥åœ¨è¿”å›å“åº”å‰è¿è¡ŒéªŒè¯é€»è¾‘ã€‚

<Note>
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`arun`æ–¹æ³•å¼‚æ­¥æ‰§è¡Œå·¥ä½œæµã€‚è¿™èƒ½åœ¨è°ƒç”¨æ™ºèƒ½ä½“æ—¶å®ç°æ›´é«˜æ•ˆçš„éé˜»å¡æ“ä½œã€‚è¯¦ç»†ç¤ºä¾‹è¯·å‚è€ƒ[å¼‚æ­¥å·¥ä½œæµç¤ºä¾‹](examples/workflows/async-hackernews-reporter)ã€‚
</Note>

## å®Œæ•´ç¤ºä¾‹ï¼šåšå®¢æ–‡ç« ç”Ÿæˆå™¨

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªèƒ½æœç´¢ç½‘ç»œã€é˜…è¯»çƒ­é—¨é“¾æ¥å¹¶æ’°å†™åšå®¢æ–‡ç« çš„å·¥ä½œæµã€‚æˆ‘ä»¬å°†åœ¨æ•°æ®åº“ä¸­ç¼“å­˜ä¸­é—´ç»“æœä»¥æé«˜æ€§èƒ½ã€‚

### åˆ›å»ºå·¥ä½œæµ

1. é€šè¿‡ç»§æ‰¿`Workflow`ç±»æ¥å®šä¹‰æ‚¨çš„å·¥ä½œæµã€‚

```python blog_post_generator.py
from agno.workflow import Workflow

class BlogPostGenerator(Workflow):
    pass
```

2. å‘å·¥ä½œæµæ·»åŠ ä¸€ä¸ªæˆ–å¤šä¸ªæ™ºèƒ½ä½“ï¼Œå¹¶åœ¨`run()`æ–¹æ³•ä¸­å®ç°å·¥ä½œæµé€»è¾‘ã€‚

```python blog_post_generator.py
import json
from textwrap import dedent
from typing import Dict, Iterator, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import RunEvent, RunResponse, Workflow
from pydantic import BaseModel, Field


class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )


class SearchResults(BaseModel):
    articles: list[NewsArticle]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )


class BlogPostGenerator(Workflow):
    """Advanced workflow for generating professional blog posts with proper research and citations."""

    description: str = dedent("""\
    An intelligent blog post generator that creates engaging, well-researched content.
    This workflow orchestrates multiple AI agents to research, analyze, and craft
    compelling blog posts that combine journalistic rigor with engaging storytelling.
    The system excels at creating content that is both informative and optimized for
    digital consumption.
    """)

    # æœç´¢æ™ºèƒ½ä½“ï¼šè´Ÿè´£æ™ºèƒ½ç½‘ç»œæœç´¢ä¸èµ„æºæ”¶é›†
    searcher: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[DuckDuckGoTools()],
        description=dedent("""\
        You are BlogResearch-X, an elite research assistant specializing in discovering
        high-quality sources for compelling blog content. Your expertise includes:

        - Finding authoritative and trending sources
        - Evaluating content credibility and relevance
        - Identifying diverse perspectives and expert opinions
        - Discovering unique angles and insights
        - Ensuring comprehensive topic coverage\
        """),
        instructions=dedent("""\
        1. Search Strategy ğŸ”
           - Find 10-15 relevant sources and select the 5-7 best ones
           - Prioritize recent, authoritative content
           - Look for unique angles and expert insights
        2. Source Evaluation ğŸ“Š
           - Verify source credibility and expertise
           - Check publication dates for timeliness
           - Assess content depth and uniqueness
        3. Diversity of Perspectives ğŸŒ
           - Include different viewpoints
           - Gather both mainstream and expert opinions
           - Find supporting data and statistics\
        """),
        response_model=SearchResults,
    )

    # å†…å®¹æŠ“å–å™¨ï¼šæå–å¹¶å¤„ç†æ–‡ç« å†…å®¹
    article_scraper: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[Newspaper4kTools()],
        description=dedent("""\
        You are ContentBot-X, a specialist in extracting and processing digital content
        for blog creation. Your expertise includes:

        - Efficient content extraction
        - Smart formatting and structuring
        - Key information identification
        - Quote and statistic preservation
        - Maintaining source attribution\
        """),
        instructions=dedent("""\
        1. Content Extraction ğŸ“‘
           - Extract content from the article
           - Preserve important quotes and statistics
           - Maintain proper attribution
           - Handle paywalls gracefully
        2. Content Processing ğŸ”„
           - Format text in clean markdown
           - Preserve key information
           - Structure content logically
        3. Quality Control âœ…
           - Verify content relevance
           - Ensure accurate extraction
           - Maintain readability\
        """),
        response_model=ScrapedArticle,
        structured_outputs=True,
    )

    # å†…å®¹å†™ä½œæ™ºèƒ½ä½“ï¼šåŸºäºç ”ç©¶åˆ›ä½œå¼•äººå…¥èƒœçš„åšå®¢æ–‡ç« 
    writer: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
        You are BlogMaster-X, an elite content creator combining journalistic excellence
        with digital marketing expertise. Your strengths include:

        - Crafting viral-worthy headlines
        - Writing engaging introductions
        - Structuring content for digital consumption
        - Incorporating research seamlessly
        - Optimizing for SEO while maintaining quality
        - Creating shareable conclusions\
        """),
        instructions=dedent("""\
        1. Content Strategy ğŸ“
           - Craft attention-grabbing headlines
           - Write compelling introductions
           - Structure content for engagement
           - Include relevant subheadings
        2. Writing Excellence âœï¸
           - Balance expertise with accessibility
           - Use clear, engaging language
           - Include relevant examples
           - Incorporate statistics naturally
        3. Source Integration ğŸ”
           - Cite sources properly
           - Include expert quotes
           - Maintain factual accuracy
        4. Digital Optimization ğŸ’»
           - Structure for scanability
           - Include shareable takeaways
           - Optimize for SEO
           - Add engaging subheadings\
        """),
        expected_output=dedent("""\
        # {ç—…æ¯’å¼ä¼ æ’­çš„æ ‡é¢˜}

        # # ç®€ä»‹
        {Engaging hook and context}

        # # {å¼•äººæ³¨ç›®çš„ç¬¬ä¸€éƒ¨åˆ†}
        {Key insights and analysis}
        {Expert quotes and statistics}

        # # {äº’åŠ¨ç¯èŠ‚ 2}
        {Deeper exploration}
        {Real-world examples}

        # # {å®è·µéƒ¨åˆ† 3}
        {Actionable insights}
        {Expert recommendations}

        # # å…³é”®è¦ç‚¹
        - {Shareable insight 1}
        - {Practical takeaway 2}
        - {Notable finding 3}

        # # æ•°æ®æº
        {Properly attributed sources with links}\
        """),
        markdown=True,
    )

    def run(
        self,
        topic: str,
        use_search_cache: bool = True,
        use_scrape_cache: bool = True,
        use_cached_report: bool = True,
    ) -> Iterator[RunResponse]:
        logger.info(f"Generating a blog post on: {topic}")

        # å¦‚æœ use_cache ä¸º True åˆ™ä½¿ç”¨ç¼“å­˜çš„åšå®¢æ–‡ç« 
        if use_cached_report:
            cached_blog_post = self.get_cached_blog_post(topic)
            if cached_blog_post:
                yield RunResponse(
                    content=cached_blog_post, event=RunEvent.workflow_completed
                )
                return

        # åœ¨ç½‘ç»œä¸Šæœç´¢å…³äºè¯¥ä¸»é¢˜çš„æ–‡ç« 
        search_results: Optional[SearchResults] = self.get_search_results(
            topic, use_search_cache
        )
        # å¦‚æœæœªæ‰¾åˆ°å…³äºè¯¥ä¸»é¢˜çš„ search_resultsï¼Œåˆ™ç»ˆæ­¢å·¥ä½œæµç¨‹
        if search_results is None or len(search_results.articles) == 0:
            yield RunResponse(
                event=RunEvent.workflow_completed,
                content=f"Sorry, could not find any articles on the topic: {topic}",
            )
            return

        # æŠ“å–æœç´¢ç»“æœ
        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(
            topic, search_results, use_scrape_cache
        )

        # ä¸ºä½œè€…å‡†å¤‡è¾“å…¥å†…å®¹
        writer_input = {
            "topic": topic,
            "articles": [v.model_dump() for v in scraped_articles.values()],
        }

        # è¿è¡Œå†™å…¥å™¨å¹¶è¿”å›å“åº”
        yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)

        # å°†åšå®¢æ–‡ç« ä¿å­˜åˆ°ç¼“å­˜ä¸­
        self.add_blog_post_to_cache(topic, self.writer.run_response.content)

    def get_cached_blog_post(self, topic: str) -> Optional[str]:
        logger.info("Checking if cached blog post exists")

        return self.session_state.get("blog_posts", {}).get(topic)

    def add_blog_post_to_cache(self, topic: str, blog_post: str):
        logger.info(f"Saving blog post for topic: {topic}")
        self.session_state.setdefault("blog_posts", {})
        self.session_state["blog_posts"][topic] = blog_post

    def get_cached_search_results(self, topic: str) -> Optional[SearchResults]:
        logger.info("Checking if cached search results exist")
        search_results = self.session_state.get("search_results", {}).get(topic)
        return (
            SearchResults.model_validate(search_results)
            if search_results and isinstance(search_results, dict)
            else search_results
        )

    def add_search_results_to_cache(self, topic: str, search_results: SearchResults):
        logger.info(f"Saving search results for topic: {topic}")
        self.session_state.setdefault("search_results", {})
        self.session_state["search_results"][topic] = search_results

    def get_cached_scraped_articles(
        self, topic: str
    ) -> Optional[Dict[str, ScrapedArticle]]:
        logger.info("Checking if cached scraped articles exist")
        scraped_articles = self.session_state.get("scraped_articles", {}).get(topic)
        return (
            ScrapedArticle.model_validate(scraped_articles)
            if scraped_articles and isinstance(scraped_articles, dict)
            else scraped_articles
        )

    def add_scraped_articles_to_cache(
        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]
    ):
        logger.info(f"Saving scraped articles for topic: {topic}")
        self.session_state.setdefault("scraped_articles", {})
        self.session_state["scraped_articles"][topic] = scraped_articles

    def get_search_results(
        self, topic: str, use_search_cache: bool, num_attempts: int = 3
    ) -> Optional[SearchResults]:
        # è‹¥ use_search_cache ä¸º Trueï¼Œåˆ™ä»ä¼šè¯çŠ¶æ€ä¸­è·å–ç¼“å­˜çš„ search_results
        if use_search_cache:
            try:
                search_results_from_cache = self.get_cached_search_results(topic)
                if search_results_from_cache is not None:
                    search_results = SearchResults.model_validate(
                        search_results_from_cache
                    )
                    logger.info(
                        f"Found {len(search_results.articles)} articles in cache."
                    )
                    return search_results
            except Exception as e:
                logger.warning(f"Could not read search results from cache: {e}")

        # å¦‚æœæ²¡æœ‰ç¼“å­˜æœç´¢ç»“æœï¼ˆsearch_resultsï¼‰ï¼Œåˆ™ä½¿ç”¨æœç´¢å™¨ï¼ˆsearcherï¼‰æŸ¥æ‰¾æœ€æ–°æ–‡ç« 
        for attempt in range(num_attempts):
            try:
                searcher_response: RunResponse = self.searcher.run(topic)
                if (
                    searcher_response is not None
                    and searcher_response.content is not None
                    and isinstance(searcher_response.content, SearchResults)
                ):
                    article_count = len(searcher_response.content.articles)
                    logger.info(
                        f"Found {article_count} articles on attempt {attempt + 1}"
                    )
                    # ç¼“å­˜æœç´¢ç»“æœ
                    self.add_search_results_to_cache(topic, searcher_response.content)
                    return searcher_response.content
                else:
                    logger.warning(
                        f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                    )
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

        logger.error(f"Failed to get search results after {num_attempts} attempts")
        return None

    def scrape_articles(
        self, topic: str, search_results: SearchResults, use_scrape_cache: bool
    ) -> Dict[str, ScrapedArticle]:
        scraped_articles: Dict[str, ScrapedArticle] = {}

        # å¦‚æœ use_scrape_cache ä¸º Trueï¼Œåˆ™ä»ä¼šè¯çŠ¶æ€è·å–ç¼“å­˜çš„ scraped_articles
        if use_scrape_cache:
            try:
                scraped_articles_from_cache = self.get_cached_scraped_articles(topic)
                if scraped_articles_from_cache is not None:
                    scraped_articles = scraped_articles_from_cache
                    logger.info(
                        f"Found {len(scraped_articles)} scraped articles in cache."
                    )
                    return scraped_articles
            except Exception as e:
                logger.warning(f"Could not read scraped articles from cache: {e}")

        # æŠ“å–ä¸åœ¨ç¼“å­˜ä¸­çš„æ–‡ç« 
        for article in search_results.articles:
            if article.url in scraped_articles:
                logger.info(f"Found scraped article in cache: {article.url}")
                continue

            article_scraper_response: RunResponse = self.article_scraper.run(
                article.url
            )
            if (
                article_scraper_response is not None
                and article_scraper_response.content is not None
                and isinstance(article_scraper_response.content, ScrapedArticle)
            ):
                scraped_articles[article_scraper_response.content.url] = (
                    article_scraper_response.content
                )
                logger.info(f"Scraped article: {article_scraper_response.content.url}")

        # å°†æŠ“å–çš„æ–‡ç« ä¿å­˜åœ¨ä¼šè¯çŠ¶æ€ä¸­
        self.add_scraped_articles_to_cache(topic, scraped_articles)
        return scraped_articles


# å¦‚æœè„šæœ¬è¢«ç›´æ¥æ‰§è¡Œï¼Œåˆ™è¿è¡Œå·¥ä½œæµç¨‹
if __name__ == "__main__":
    import random

    from rich.prompt import Prompt

    # å±•ç¤ºç”Ÿæˆå™¨å¤šåŠŸèƒ½æ€§çš„è¶£å‘³ç¤ºä¾‹æç¤º
    example_prompts = [
        "Why Cats Secretly Run the Internet",
        "The Science Behind Why Pizza Tastes Better at 2 AM",
        "Time Travelers' Guide to Modern Social Media",
        "How Rubber Ducks Revolutionized Software Development",
        "The Secret Society of Office Plants: A Survival Guide",
        "Why Dogs Think We're Bad at Smelling Things",
        "The Underground Economy of Coffee Shop WiFi Passwords",
        "A Historical Analysis of Dad Jokes Through the Ages",
    ]

    # ä»ç”¨æˆ·å¤„è·å–ä¸»é¢˜
    topic = Prompt.ask(
        "[bold]Enter a blog post topic[/bold] (or press Enter for a random example)\nâœ¨",
        default=random.choice(example_prompts),
    )

    # å°†ä¸»é¢˜è½¬æ¢ä¸ºURLå®‰å…¨çš„å­—ç¬¦ä¸²ç”¨äºsession_id
    url_safe_topic = topic.lower().replace(" ", "-")

    # åˆå§‹åŒ–åšå®¢æ–‡ç« ç”Ÿæˆå™¨å·¥ä½œæµç¨‹
    # - æ ¹æ®ä¸»é¢˜ç”Ÿæˆå”¯ä¸€çš„ä¼šè¯ID
    # - è®¾ç½® SQLite å­˜å‚¨ç”¨äºç¼“å­˜ç»“æœ
    generate_blog_post = BlogPostGenerator(
        session_id=f"generate-blog-post-on-{url_safe_topic}",
        storage=SqliteStorage(
            table_name="generate_blog_post_workflows",
            db_file="tmp/agno_workflows.db",
        ),
        debug_mode=True,
    )

    # å¯ç”¨ç¼“å­˜æ‰§è¡Œå·¥ä½œæµ
    # è¿”å›ä¸€ä¸ªåŒ…å«ç”Ÿæˆå†…å®¹çš„RunResponseå¯¹è±¡è¿­ä»£å™¨
    blog_post: Iterator[RunResponse] = generate_blog_post.run(
        topic=topic,
        use_search_cache=True,
        use_scrape_cache=True,
        use_cached_report=True,
    )

    # æ‰“å°å“åº”å†…å®¹
    pprint_run_response(blog_post, markdown=True)
```

### è¿è¡Œå·¥ä½œæµ

å®‰è£…åº“

```shell
pip install agno openai duckduckgo-search sqlalchemy
```

è¿è¡Œå·¥ä½œæµ

```shell
python blog_post_generator.py
```

ç°åœ¨ç»“æœå·²ç¼“å­˜è‡³æ•°æ®åº“ï¼Œå¯ä¾›åç»­è¿è¡Œé‡å¤ä½¿ç”¨ã€‚å†æ¬¡è¿è¡Œå·¥ä½œæµå¯æŸ¥çœ‹ç¼“å­˜ç»“æœã€‚

```shell
python blog_post_generator.py
```

<img
  height="200"
  src="/images/BlogPostGenerator.gif"
  style={{ borderRadius: '8px' }}
/>

æŸ¥çœ‹æ›´å¤š[ç”¨ä¾‹](/examples/workflows/)å’Œ[ç¤ºä¾‹](/examples/concepts/storage/workflow_storage)ã€‚

## è®¾è®¡å†³ç­–

<Tip>

**ä¸ºä»€ä¹ˆæˆ‘ä»¬å»ºè®®ç”¨Pythonå‡½æ•°ç¼–å†™å·¥ä½œæµé€»è¾‘ï¼Œè€Œä¸æ˜¯åˆ›å»ºGraphã€Chainæˆ–Flowç­‰è‡ªå®šä¹‰æŠ½è±¡ï¼Ÿ**

åœ¨æˆ‘ä»¬æ„å»ºAIäº§å“çš„ç»éªŒä¸­ï¼Œå·¥ä½œæµé€»è¾‘éœ€è¦åŠ¨æ€åŒ–ï¼ˆå³åœ¨è¿è¡Œæ—¶ç¡®å®šï¼‰ï¼Œå¹¶è¦æ±‚å¯¹å¹¶è¡ŒåŒ–ã€ç¼“å­˜ã€çŠ¶æ€ç®¡ç†ã€é”™è¯¯å¤„ç†å’Œé—®é¢˜è§£å†³è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚

ä½¿ç”¨å¸¦æœ‰æ–°DSLçš„è‡ªå®šä¹‰æŠ½è±¡ï¼ˆGraphã€Chainã€Flowï¼‰æ„å‘³ç€å­¦ä¹ æ–°æ¦‚å¿µå¹¶ç¼–å†™æ›´å¤šä»£ç ã€‚æœ€ç»ˆæˆ‘ä»¬ä¼šèŠ±è´¹æ›´å¤šæ—¶é—´å­¦ä¹ å’Œå¯¹æŠ—DSLã€‚

åœ¨æ¯ä¸ªé¡¹ç›®ä¸­ï¼Œç®€å•çš„Pythonå‡½æ•°æ€»èƒ½è§£å†³é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œå¤æ‚å·¥ä½œæµå¯ä»¥è·¨å¤šä¸ªæ–‡ä»¶ï¼Œæœ‰æ—¶ä¼šè‡ªæˆæ¨¡å—ã€‚æ‚¨çŸ¥é“ä»€ä¹ˆæœ€é€‚åˆè¿™ç§æƒ…å†µå—ï¼ŸPythonã€‚

æˆ‘ä»¬å§‹ç»ˆå›å½’[Unixå“²å­¦](https://en.wikipedia.org/wiki/Unix_philosophy)ã€‚

å¦‚æœæˆ‘ä»¬çš„å·¥ä½œæµæ— æ³•ç”¨åŸç”ŸPythonç¼–å†™ï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥ç®€åŒ–å’Œé‡ç»„å·¥ä½œæµï¼Œè€Œä¸æ˜¯åè¿‡æ¥ã€‚

é•¿æœŸè¿è¡Œå·¥ä½œæµçš„å¦ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜æ˜¯ç®¡ç†è¯·æ±‚/å“åº”è¶…æ—¶ã€‚æˆ‘ä»¬éœ€è¦å·¥ä½œæµå¼‚æ­¥è§¦å‘ã€å‘å®¢æˆ·ç«¯ç¡®è®¤å¯åŠ¨ï¼Œç„¶åå…è®¸å®¢æˆ·ç«¯ç¨åè½®è¯¢ç»“æœã€‚å®ç°è¿™ç§ç”¨æˆ·ä½“éªŒéœ€è¦åœ¨åå°ä»»åŠ¡ä¸­è¿è¡Œå·¥ä½œæµï¼Œå¹¶ä¸¥æ ¼ç®¡ç†çŠ¶æ€ä»¥ç¡®ä¿å®¢æˆ·ç«¯èƒ½è·å–æœ€æ–°æ›´æ–°ã€‚

åŸºäºè¿™äº›åŸå› ï¼Œæˆ‘ä»¬å»ºè®®ç”¨åŸç”ŸPythonå‡½æ•°æ„å»ºå·¥ä½œæµï¼Œå…¶æ§åˆ¶çº§åˆ«ã€çµæ´»æ€§å’Œå¯é æ€§æ— å¯åŒ¹æ•Œã€‚

</Tip>