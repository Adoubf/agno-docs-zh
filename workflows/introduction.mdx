---
title: 什么是工作流？
sidebarTitle: 概览
---
工作流是专为生产应用设计的确定性、有状态、多智能体程序。它们经过实战检验，功能极其强大，并提供以下优势：

- **纯Python**：使用标准Python构建工作流逻辑。在开发了数百个智能体系统后，**没有任何框架或基于步骤的方法能比纯Python提供更高的灵活性和可靠性**。想要循环？使用while/for；想要条件判断？使用if/else；想要异常处理？使用try/except。
- **完全控制与灵活性**：由于工作流逻辑是Python函数，您可以完全掌控整个流程，例如在处理前验证输入、并行生成和运行智能体、按需缓存结果以及纠正中间错误。**这种控制级别对可靠性至关重要。**
- **内置存储与缓存**：工作流自带存储和状态管理功能。使用session_state缓存中间结果。这种方法的一大优势是，您可以在单独进程中触发工作流并稍后查询结果，从而避免长时间运行工作流常见的请求超时问题。

<Check>

由于工作流逻辑是Python函数，AI代码编辑器可以为您编写工作流。只需添加`https://docs.agno.com`作为文档源。

</Check>

### 最大亮点

无需学习新知识！您已经掌握Python，已经知道如何构建智能体和团队——现在只需用常规Python代码将它们组合起来。不必学习新的DSL或语法。

这是一个缓存输出的简单工作流示例。您可以看到对流程的控制级别，甚至"存储状态"都是在响应生成后进行的。

```python simple_cache_workflow.py
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import Workflow


class CacheWorkflow(Workflow):
    # 纯描述性内容，不用于工作流程
    description: str = "A workflow that caches previous outputs"

    # 在工作流中将智能体或团队添加为属性
    agent = Agent(model=OpenAIChat(id="gpt-4o-mini"))

    # 在 `run()` 方法中编写逻辑
    def run(self, message: str) -> Iterator[RunResponse]:
        logger.info(f"Checking cache for '{message}'")
        # 检查输出是否已缓存
        if self.session_state.get(message):
            logger.info(f"Cache hit for '{message}'")
            yield RunResponse(run_id=self.run_id, content=self.session_state.get(message))
            return

        logger.info(f"Cache miss for '{message}'")
        # 运行智能体并返回响应
        yield from self.agent.run(message, stream=True)

        # 在响应生成后缓存输出
        self.session_state[message] = self.agent.run_response.content


if __name__ == "__main__":
    workflow = CacheWorkflow()
    # 运行工作流（耗时约1秒）
    response: Iterator[RunResponse] = workflow.run(message="Tell me a joke.")
    # 输出响应
    pprint_run_response(response, markdown=True, show_time=True)
    # 再次运行工作流程（由于缓存机制会立即执行）
    response: Iterator[RunResponse] = workflow.run(message="Tell me a joke.")
    # 输出响应内容
    pprint_run_response(response, markdown=True, show_time=True)
```

### 如何构建工作流

1. 通过继承`Workflow`类来定义您的工作流类。
2. 将智能体或团队添加为工作流的属性。这不是严格要求的，但有助于我们将智能体的session_id映射到工作流的session_id。
3. 在`run()`方法中实现工作流逻辑。这是运行工作流时调用的主要函数（**工作流入口点**）。该函数赋予我们对流程的极大控制权：部分智能体可以流式传输，其他可以生成结构化输出，智能体可以使用`async.gather()`并行运行，某些智能体可以在返回响应前运行验证逻辑。

<Note>
您还可以使用`arun`方法异步执行工作流。这能在调用智能体时实现更高效的非阻塞操作。详细示例请参考[异步工作流示例](examples/workflows/async-hackernews-reporter)。
</Note>

## 完整示例：博客文章生成器

让我们创建一个能搜索网络、阅读热门链接并撰写博客文章的工作流。我们将在数据库中缓存中间结果以提高性能。

### 创建工作流

1. 通过继承`Workflow`类来定义您的工作流。

```python blog_post_generator.py
from agno.workflow import Workflow

class BlogPostGenerator(Workflow):
    pass
```

2. 向工作流添加一个或多个智能体，并在`run()`方法中实现工作流逻辑。

```python blog_post_generator.py
import json
from textwrap import dedent
from typing import Dict, Iterator, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import RunEvent, RunResponse, Workflow
from pydantic import BaseModel, Field


class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )


class SearchResults(BaseModel):
    articles: list[NewsArticle]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )


class BlogPostGenerator(Workflow):
    """Advanced workflow for generating professional blog posts with proper research and citations."""

    description: str = dedent("""\
    An intelligent blog post generator that creates engaging, well-researched content.
    This workflow orchestrates multiple AI agents to research, analyze, and craft
    compelling blog posts that combine journalistic rigor with engaging storytelling.
    The system excels at creating content that is both informative and optimized for
    digital consumption.
    """)

    # 搜索智能体：负责智能网络搜索与资源收集
    searcher: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[DuckDuckGoTools()],
        description=dedent("""\
        You are BlogResearch-X, an elite research assistant specializing in discovering
        high-quality sources for compelling blog content. Your expertise includes:

        - Finding authoritative and trending sources
        - Evaluating content credibility and relevance
        - Identifying diverse perspectives and expert opinions
        - Discovering unique angles and insights
        - Ensuring comprehensive topic coverage\
        """),
        instructions=dedent("""\
        1. Search Strategy 🔍
           - Find 10-15 relevant sources and select the 5-7 best ones
           - Prioritize recent, authoritative content
           - Look for unique angles and expert insights
        2. Source Evaluation 📊
           - Verify source credibility and expertise
           - Check publication dates for timeliness
           - Assess content depth and uniqueness
        3. Diversity of Perspectives 🌐
           - Include different viewpoints
           - Gather both mainstream and expert opinions
           - Find supporting data and statistics\
        """),
        response_model=SearchResults,
    )

    # 内容抓取器：提取并处理文章内容
    article_scraper: Agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[Newspaper4kTools()],
        description=dedent("""\
        You are ContentBot-X, a specialist in extracting and processing digital content
        for blog creation. Your expertise includes:

        - Efficient content extraction
        - Smart formatting and structuring
        - Key information identification
        - Quote and statistic preservation
        - Maintaining source attribution\
        """),
        instructions=dedent("""\
        1. Content Extraction 📑
           - Extract content from the article
           - Preserve important quotes and statistics
           - Maintain proper attribution
           - Handle paywalls gracefully
        2. Content Processing 🔄
           - Format text in clean markdown
           - Preserve key information
           - Structure content logically
        3. Quality Control ✅
           - Verify content relevance
           - Ensure accurate extraction
           - Maintain readability\
        """),
        response_model=ScrapedArticle,
        structured_outputs=True,
    )

    # 内容写作智能体：基于研究创作引人入胜的博客文章
    writer: Agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        description=dedent("""\
        You are BlogMaster-X, an elite content creator combining journalistic excellence
        with digital marketing expertise. Your strengths include:

        - Crafting viral-worthy headlines
        - Writing engaging introductions
        - Structuring content for digital consumption
        - Incorporating research seamlessly
        - Optimizing for SEO while maintaining quality
        - Creating shareable conclusions\
        """),
        instructions=dedent("""\
        1. Content Strategy 📝
           - Craft attention-grabbing headlines
           - Write compelling introductions
           - Structure content for engagement
           - Include relevant subheadings
        2. Writing Excellence ✍️
           - Balance expertise with accessibility
           - Use clear, engaging language
           - Include relevant examples
           - Incorporate statistics naturally
        3. Source Integration 🔍
           - Cite sources properly
           - Include expert quotes
           - Maintain factual accuracy
        4. Digital Optimization 💻
           - Structure for scanability
           - Include shareable takeaways
           - Optimize for SEO
           - Add engaging subheadings\
        """),
        expected_output=dedent("""\
        # {病毒式传播的标题}

        # # 简介
        {Engaging hook and context}

        # # {引人注目的第一部分}
        {Key insights and analysis}
        {Expert quotes and statistics}

        # # {互动环节 2}
        {Deeper exploration}
        {Real-world examples}

        # # {实践部分 3}
        {Actionable insights}
        {Expert recommendations}

        # # 关键要点
        - {Shareable insight 1}
        - {Practical takeaway 2}
        - {Notable finding 3}

        # # 数据源
        {Properly attributed sources with links}\
        """),
        markdown=True,
    )

    def run(
        self,
        topic: str,
        use_search_cache: bool = True,
        use_scrape_cache: bool = True,
        use_cached_report: bool = True,
    ) -> Iterator[RunResponse]:
        logger.info(f"Generating a blog post on: {topic}")

        # 如果 use_cache 为 True 则使用缓存的博客文章
        if use_cached_report:
            cached_blog_post = self.get_cached_blog_post(topic)
            if cached_blog_post:
                yield RunResponse(
                    content=cached_blog_post, event=RunEvent.workflow_completed
                )
                return

        # 在网络上搜索关于该主题的文章
        search_results: Optional[SearchResults] = self.get_search_results(
            topic, use_search_cache
        )
        # 如果未找到关于该主题的 search_results，则终止工作流程
        if search_results is None or len(search_results.articles) == 0:
            yield RunResponse(
                event=RunEvent.workflow_completed,
                content=f"Sorry, could not find any articles on the topic: {topic}",
            )
            return

        # 抓取搜索结果
        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(
            topic, search_results, use_scrape_cache
        )

        # 为作者准备输入内容
        writer_input = {
            "topic": topic,
            "articles": [v.model_dump() for v in scraped_articles.values()],
        }

        # 运行写入器并返回响应
        yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)

        # 将博客文章保存到缓存中
        self.add_blog_post_to_cache(topic, self.writer.run_response.content)

    def get_cached_blog_post(self, topic: str) -> Optional[str]:
        logger.info("Checking if cached blog post exists")

        return self.session_state.get("blog_posts", {}).get(topic)

    def add_blog_post_to_cache(self, topic: str, blog_post: str):
        logger.info(f"Saving blog post for topic: {topic}")
        self.session_state.setdefault("blog_posts", {})
        self.session_state["blog_posts"][topic] = blog_post

    def get_cached_search_results(self, topic: str) -> Optional[SearchResults]:
        logger.info("Checking if cached search results exist")
        search_results = self.session_state.get("search_results", {}).get(topic)
        return (
            SearchResults.model_validate(search_results)
            if search_results and isinstance(search_results, dict)
            else search_results
        )

    def add_search_results_to_cache(self, topic: str, search_results: SearchResults):
        logger.info(f"Saving search results for topic: {topic}")
        self.session_state.setdefault("search_results", {})
        self.session_state["search_results"][topic] = search_results

    def get_cached_scraped_articles(
        self, topic: str
    ) -> Optional[Dict[str, ScrapedArticle]]:
        logger.info("Checking if cached scraped articles exist")
        scraped_articles = self.session_state.get("scraped_articles", {}).get(topic)
        return (
            ScrapedArticle.model_validate(scraped_articles)
            if scraped_articles and isinstance(scraped_articles, dict)
            else scraped_articles
        )

    def add_scraped_articles_to_cache(
        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]
    ):
        logger.info(f"Saving scraped articles for topic: {topic}")
        self.session_state.setdefault("scraped_articles", {})
        self.session_state["scraped_articles"][topic] = scraped_articles

    def get_search_results(
        self, topic: str, use_search_cache: bool, num_attempts: int = 3
    ) -> Optional[SearchResults]:
        # 若 use_search_cache 为 True，则从会话状态中获取缓存的 search_results
        if use_search_cache:
            try:
                search_results_from_cache = self.get_cached_search_results(topic)
                if search_results_from_cache is not None:
                    search_results = SearchResults.model_validate(
                        search_results_from_cache
                    )
                    logger.info(
                        f"Found {len(search_results.articles)} articles in cache."
                    )
                    return search_results
            except Exception as e:
                logger.warning(f"Could not read search results from cache: {e}")

        # 如果没有缓存搜索结果（search_results），则使用搜索器（searcher）查找最新文章
        for attempt in range(num_attempts):
            try:
                searcher_response: RunResponse = self.searcher.run(topic)
                if (
                    searcher_response is not None
                    and searcher_response.content is not None
                    and isinstance(searcher_response.content, SearchResults)
                ):
                    article_count = len(searcher_response.content.articles)
                    logger.info(
                        f"Found {article_count} articles on attempt {attempt + 1}"
                    )
                    # 缓存搜索结果
                    self.add_search_results_to_cache(topic, searcher_response.content)
                    return searcher_response.content
                else:
                    logger.warning(
                        f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                    )
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

        logger.error(f"Failed to get search results after {num_attempts} attempts")
        return None

    def scrape_articles(
        self, topic: str, search_results: SearchResults, use_scrape_cache: bool
    ) -> Dict[str, ScrapedArticle]:
        scraped_articles: Dict[str, ScrapedArticle] = {}

        # 如果 use_scrape_cache 为 True，则从会话状态获取缓存的 scraped_articles
        if use_scrape_cache:
            try:
                scraped_articles_from_cache = self.get_cached_scraped_articles(topic)
                if scraped_articles_from_cache is not None:
                    scraped_articles = scraped_articles_from_cache
                    logger.info(
                        f"Found {len(scraped_articles)} scraped articles in cache."
                    )
                    return scraped_articles
            except Exception as e:
                logger.warning(f"Could not read scraped articles from cache: {e}")

        # 抓取不在缓存中的文章
        for article in search_results.articles:
            if article.url in scraped_articles:
                logger.info(f"Found scraped article in cache: {article.url}")
                continue

            article_scraper_response: RunResponse = self.article_scraper.run(
                article.url
            )
            if (
                article_scraper_response is not None
                and article_scraper_response.content is not None
                and isinstance(article_scraper_response.content, ScrapedArticle)
            ):
                scraped_articles[article_scraper_response.content.url] = (
                    article_scraper_response.content
                )
                logger.info(f"Scraped article: {article_scraper_response.content.url}")

        # 将抓取的文章保存在会话状态中
        self.add_scraped_articles_to_cache(topic, scraped_articles)
        return scraped_articles


# 如果脚本被直接执行，则运行工作流程
if __name__ == "__main__":
    import random

    from rich.prompt import Prompt

    # 展示生成器多功能性的趣味示例提示
    example_prompts = [
        "Why Cats Secretly Run the Internet",
        "The Science Behind Why Pizza Tastes Better at 2 AM",
        "Time Travelers' Guide to Modern Social Media",
        "How Rubber Ducks Revolutionized Software Development",
        "The Secret Society of Office Plants: A Survival Guide",
        "Why Dogs Think We're Bad at Smelling Things",
        "The Underground Economy of Coffee Shop WiFi Passwords",
        "A Historical Analysis of Dad Jokes Through the Ages",
    ]

    # 从用户处获取主题
    topic = Prompt.ask(
        "[bold]Enter a blog post topic[/bold] (or press Enter for a random example)\n✨",
        default=random.choice(example_prompts),
    )

    # 将主题转换为URL安全的字符串用于session_id
    url_safe_topic = topic.lower().replace(" ", "-")

    # 初始化博客文章生成器工作流程
    # - 根据主题生成唯一的会话ID
    # - 设置 SQLite 存储用于缓存结果
    generate_blog_post = BlogPostGenerator(
        session_id=f"generate-blog-post-on-{url_safe_topic}",
        storage=SqliteStorage(
            table_name="generate_blog_post_workflows",
            db_file="tmp/agno_workflows.db",
        ),
        debug_mode=True,
    )

    # 启用缓存执行工作流
    # 返回一个包含生成内容的RunResponse对象迭代器
    blog_post: Iterator[RunResponse] = generate_blog_post.run(
        topic=topic,
        use_search_cache=True,
        use_scrape_cache=True,
        use_cached_report=True,
    )

    # 打印响应内容
    pprint_run_response(blog_post, markdown=True)
```

### 运行工作流

安装库

```shell
pip install agno openai duckduckgo-search sqlalchemy
```

运行工作流

```shell
python blog_post_generator.py
```

现在结果已缓存至数据库，可供后续运行重复使用。再次运行工作流可查看缓存结果。

```shell
python blog_post_generator.py
```

<img
  height="200"
  src="/images/BlogPostGenerator.gif"
  style={{ borderRadius: '8px' }}
/>

查看更多[用例](/examples/workflows/)和[示例](/examples/concepts/storage/workflow_storage)。

## 设计决策

<Tip>

**为什么我们建议用Python函数编写工作流逻辑，而不是创建Graph、Chain或Flow等自定义抽象？**

在我们构建AI产品的经验中，工作流逻辑需要动态化（即在运行时确定），并要求对并行化、缓存、状态管理、错误处理和问题解决进行细粒度控制。

使用带有新DSL的自定义抽象（Graph、Chain、Flow）意味着学习新概念并编写更多代码。最终我们会花费更多时间学习和对抗DSL。

在每个项目中，简单的Python函数总能解决问题。我们还发现，复杂工作流可以跨多个文件，有时会自成模块。您知道什么最适合这种情况吗？Python。

我们始终回归[Unix哲学](https://en.wikipedia.org/wiki/Unix_philosophy)。

如果我们的工作流无法用原生Python编写，那么我们应该简化和重组工作流，而不是反过来。

长期运行工作流的另一个重大挑战是管理请求/响应超时。我们需要工作流异步触发、向客户端确认启动，然后允许客户端稍后轮询结果。实现这种用户体验需要在后台任务中运行工作流，并严格管理状态以确保客户端能获取最新更新。

基于这些原因，我们建议用原生Python函数构建工作流，其控制级别、灵活性和可靠性无可匹敌。

</Tip>