---
title: Build an LLM Api
---

Use this template to build an **LLM Api** powered by GPT-4.

We'll use **Postgres** and **FastApi**, a well-loved stack for building RestAPIs.

<Note>

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) to run this app locally.

</Note>

## Setup

Open the `Terminal` and create an `ai` directory with a python virtual environment.

<CodeGroup>

```bash Mac
mkdir ai && cd ai

python3 -m venv aienv
source aienv/bin/activate
```

```bash Windows
mkdir ai; cd ai

python3 -m venv aienv
aienv/scripts/activate
```

</CodeGroup>

Install `phidata`

<CodeGroup>

```bash Mac
pip install -U phidata
```

```bash Windows
pip install -U phidata
```

</CodeGroup>

<Note>

If you encounter errors, update pip using `python -m pip install --upgrade pip`

</Note>

## Create your codebase

Create your codebase using the `llm-api` template pre-configured with [FastApi](https://fastapi.tiangolo.com/) and [Postgres](https://www.postgresql.org/). We'll use this codebase as the starting point for our LLM product.

<CodeGroup>

```bash Mac
phi ws create -t llm-api -n llm-api
```

```bash Windows
phi ws create -t llm-api -n llm-api
```

</CodeGroup>

This will create a folder named `llm-api` with the following structure:

```bash llm-api
llm-api                     # root directory for your llm-api
├── api                     # directory for FastApi routes
├── db                      # directory for database components
├── llm                     # directory for LLM components
    ├── conversations       # LLM conversations
    ├── knowledge_base.py   # LLM knowledge base
    └── storage.py          # LLM storage
├── Dockerfile              # Dockerfile for the application
├── pyproject.toml          # python project definition
├── requirements.txt        # python dependencies generated by pyproject.toml
├── scripts                 # directory for helper scripts
├── tests                   # directory for unit tests
├── utils                   # directory for shared utilities
└── workspace               # phidata workspace directory
    ├── dev_resources.py    # dev resources running locally
    ├── prd_resources.py    # production resources running on AWS
    ├── secrets             # directory for storing secrets
    └── settings.py         # phidata workspace settings
```

### Optional: Set OpenAI Api Key

To use your own `OPENAI_API_KEY`, set the `OPENAI_API_KEY` environment variable. You can get one [from OpenAI here](https://platform.openai.com/account/api-keys). Otherwise `phi` provides ready to use access to OpenAI models.

<CodeGroup>

```bash Mac
export OPENAI_API_KEY=sk-***
```

```bash Windows
setx OPENAI_API_KEY sk-***
```

</CodeGroup>

## Local LLM Api

[FastApi](https://fastapi.tiangolo.com/) is an exceptional
framework for building RestApis. Its fast, well-designed and well-loved by everyone
using it. Your codebase comes pre-configured with [FastApi](https://fastapi.tiangolo.com/) and [Postgres](https://www.postgresql.org/).
Run it using:

<CodeGroup>

```bash terminal
phi ws up
```

```bash full options
phi ws up --env dev --infra docker
```

```bash shorthand
phi ws up dev:docker
```

</CodeGroup>

**Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.

### View API Endpoints

- Open [localhost:8000/docs](http://localhost:8000/docs) to view the API Endpoints.
- Load the knowledge base using `v1/pdf/conversation/load-knowledge-base`
- Test the `v1/pdf/conversation/chat` endpoint with `{"message": "How do I make chicken curry?"}`

<ImageContainer
  src="/images/llm-api-fastapi-local.png"
  alt="llm-api-fastapi-local"
/>

## Add your data

The `pdf_knowledge_base` in the `llm/knowledge_base.py` file provides the context used by the LLM. The `PDFKnowledgeBase` reads local PDFs and the `PDFUrlKnowledgeBase` reads PDFs from URLs.

<Note>

**Make this app your own** by creating a `data/pdfs` folder with your PDFs. Then click on the `Update Knowledge Base` button to load the knowledge base.

</Note>

```python llm/knowledge_base.py
url_pdf_knowledge_base = PDFUrlKnowledgeBase(
  ...
)

local_pdf_knowledge_base = PDFKnowledgeBase(
    path="data/pdfs",
    ...
)

pdf_knowledge_base = CombinedKnowledgeBase(
    sources=[
        url_pdf_knowledge_base,
        local_pdf_knowledge_base,
    ],
    num_documents=2, # 2 references are added to the prompt.
    ...
)
```

## Build your LLM Product

Your LLM Api comes with common endpoints you can use to build your LLM product. These routes are developed in collaboration with real LLM Apps and provide a great starting point to build on.

For example:

- Call the `/conversation/create` endpoint to create a new conversation for a user.

```json
{
  "user_name": "my-app-user-1",
  "conversation_type": "RAG"
}
```

- The response contains a `conversation_id` that can be used to build a chat interface by calling the `/conversation/chat` endpoint. Message us on [discord](https://discord.gg/4MtYHHrgA8) if you need help.

```json
{
  "message": "how do I make pasta",
  "stream": true,
  "conversation_id": "fc67f202-c2cb-4c3d-a640-704ab83d9460",
  "conversation_type": "RAG"
}
```

The FastApi routes are defined the `api/routes` folder and can be customized to your use case.

## Delete local resources

Play around and stop the workspace using:

<CodeGroup>

```bash terminal
phi ws down
```

```bash full options
phi ws down --env dev --infra docker
```

```bash shorthand
phi ws down dev:docker
```

</CodeGroup>

---

## Run on AWS

Now let's run the **LLM API** in production on AWS.

### AWS Authentication

To run on AWS, you need **one** of the following:

1. The `~/.aws/credentials` file with your AWS credentials
2. `AWS_ACCESS_KEY_ID` + `AWS_SECRET_ACCESS_KEY` environment variables

<Note>

To create the credentials file, install the [aws cli](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) and run `aws configure`

</Note>

### Add AWS Region and Subnets

Add 2 [Subnets](https://us-east-1.console.aws.amazon.com/vpc/home?#subnets:) to the `workspace/settings.py` file, these are required for the ECS services.

```python workspace/settings.py
ws_settings = WorkspaceSettings(
    ...
    # -*- AWS settings
    # Add your Subnet IDs here
    subnet_ids=["subnet-xyz", "subnet-xyz"],
    ...
)
```

<Note>

Please check that the subnets belong to the selected `aws_region`

</Note>

## Update Secrets

Update the RDS database password in the `workspace/secrets/prd_db_secrets.yml` file:

```python workspace/secrets/prd_db_secrets.yml
# Secrets used by prd RDS database
MASTER_USERNAME: api
MASTER_USER_PASSWORD: "api9999!!"
```

## Create AWS resources

Create AWS resources for the LLM Api using:

<CodeGroup>

```bash terminal
phi ws up --env prd --infra aws
```

```bash shorthand
phi ws up prd:aws
```

</CodeGroup>

This will create:

1. [ECS Cluster](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/clusters.html) for running the application.
2. [ECS Task Definitions](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html) and [Services](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html) that run the application on the ECS cluster.
3. [LoadBalancer](https://aws.amazon.com/elasticloadbalancing/) to route traffic to the application.
4. [Security Groups](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) that control incoming and outgoing traffic.
5. [Secrets](https://aws.amazon.com/secrets-manager/) for managing application and database secrets.
6. [RDS Database](https://aws.amazon.com/rds/) for Knowledge Base and Storage.

**Press Enter** to confirm and grab a cup of coffee while the resources spin up.

- The RDS database takes about 10 minutes to activate.
- These resources are defined in the `workspace/prd_resources.py` file.
- Use the [ECS console](https://us-east-1.console.aws.amazon.com/ecs/v2/clusters) to view services and logs.
- Use the [RDS console](https://us-east-1.console.aws.amazon.com/rds/home?#databases:) to view the database instance.

## Production LLM Api

- **Open the LoadBalancer DNS** + the `/docs` endpoint to view the API Endpoints.
- Load the knowledge base using `/v1/pdf/conversation/load-knowledge-base`
- Test the `v1/pdf/conversation/chat` endpoint with `{"message": "How do I make chicken curry?"}`
- Update and integrate with your front-end or product.

<ImageContainer
  src="/images/llm-api-fastapi-prd.png"
  alt="llm-api-fastapi-prd"
/>

## Update Production

To update the production application, follow [this guide](/how-to/production-app) to

1. Create a new image
2. Update the ECS Task Definition and Services.

## Delete AWS resources

Play around and then delete AWS resources using:

<CodeGroup>

```bash terminal
phi ws down --env prd --infra aws
```

```bash shorthand
phi ws down prd:aws
```

</CodeGroup>

or delete individual resource groups using:

<CodeGroup>

```bash Api
phi ws down --env prd --infra aws --group api
```

```bash Database
phi ws down --env prd --infra aws --group db
```

</CodeGroup>

## Next

Congratulations on building a prodution-ready LLM Api. Next Steps:

- Make this App your own by adding your data.
- Create a [git repository for this workspace](/how-to/git-repo).
- Read how to [update the dev application](/how-to/dev-app).
- Read how to [update the production application](/how-to/production-app).
- Read how to [format and validate your code](/how-to/format-and-validate).
- Read how to [add python libraries](/how-to/python-libraries).
- Read how to [add database tables](/how-to/database-tables)
- Read how to [add a custom domain and HTTPS](/how-to/domain-https).
- Read how to [implement CI/CD](/how-to/ci-cd)
- Learn about other [day-2 operations](/how-to/how-to-operations).
- Chat with us on [discord](https://discord.gg/4MtYHHrgA8).

---
