---
title: "Build an LLM App"
sidebarTitle: "Run locally"
---

Use this template to build an **LLM App** powered by GPT-4. This template comes pre-configured with **PgVector** for Knowledge Base and Storage. We'll serve the app using **Streamlit** and **FastApi**, running locally on docker and in production on AWS.

**By the end of this guide you'll be up and running with your own LLM App.**

<Snippet file="setup.mdx" />

<Snippet file="create-llm-app-codebase.mdx" />

<Snippet file="set-openai-key.mdx" />

<Snippet file="serve-llm-app-streamlit.mdx" />

<Snippet file="chat-with-pdfs-rag.mdx" />

<Snippet file="llm-app-add-data.mdx" />

## How this App works

The streamlit apps are defined in the `app` folder and the `Conversations` powering these apps are defined in the `llm/conversations` folder.

**Checkout the `llm/conversations/pdf_rag.py` file for the `RAG` conversation:**

- The `add_references_to_prompt` flag will populate the `references` argument of the `user_prompt_function`
- The `user_prompt_function` builds the **Prompt** sent to the LLM. The prompt is injected with additional information from the knowledge base using the `references` variable.

```python llm/conversations/pdf_rag.py
...
def get_pdf_rag_conversation(
    user_name: Optional[str] = None,
    conversation_id: Optional[str] = None,
    debug_mode: bool = False,
) -> Conversation:
    """Get a RAG conversation with the PDF knowledge base"""

    return Conversation(
        ...
        user_prompt_function=lambda message, references, **kwargs: f"""\
        Use the following information from the knowledge base if it helps.
        <knowledge_base>
        {references}
        </knowledge_base>

        Respond to the following message:
        USER: {message}
        ASSISTANT:
        """,
        # This populates the "references" argument to the user prompt function
        add_references_to_prompt=True,
        # This adds the last 8 messages to the API call
        add_chat_history_to_messages=True,
    )
```

**Checkout the `llm/conversations/pdf_auto.py` file for the `Autonomous` conversation:**

- The `function_calls` flag gives the LLM the ability to call functions.
- The `show_function_calls` prints which functions the LLM is running.

```python llm/conversations/pdf_rag.py
...
def get_pdf_auto_conversation(
    user_name: Optional[str] = None,
    conversation_id: Optional[str] = None,
    debug_mode: bool = False,
) -> Conversation:
    """Get an autonomous conversation with the PDF knowledge base"""

    return Conversation(
        ...
        function_calls=True,
        show_function_calls=True,
        ...
    )
```

<Snippet file="serve-llm-app-fastapi.mdx" />

### View API Endpoints

- Open [localhost:8000/docs](http://localhost:8000/docs) to view the API Endpoints.
- Load the knowledge base using `/v1/pdf/conversation/load-knowledge-base`
- Test the `v1/pdf/conversation/chat` endpoint with

```json
{
  "message": "How do I make chicken curry?"
}
```

![rag-llm-app-fastapi-local](/images/rag-llm-app-fastapi-local.png)

## Build your LLM Product

Your LLM Api provides common endpoints you can use to build your LLM product. These routes are developed in collaboration with real AI Apps and are a great starting point to build on.

For example:

- Call the `/conversation/create` endpoint to create a new conversation for a user.

```json
{
  "user_name": "my-app-user-1",
  "conversation_type": "RAG"
}
```

- The response contains a `conversation_id` that can be used to build a chat interface by calling the `/conversation/chat` endpoint.

```json
{
  "message": "how do I make pasta",
  "stream": true,
  "conversation_id": "fc67f202-c2cb-4c3d-a640-704ab83d9460",
  "conversation_type": "RAG"
}
```

These routes are defined the `api/routes` folder and can be customized to your use case.

<Tip>
  Message us on [discord](https://discord.gg/4MtYHHrgA8) if you need help.
</Tip>

<Snippet file="llm-app-run-jupyter.mdx" />

<Snippet file="llm-app-delete-local-resources.mdx" />

## Next

Congratulations on running your LLM App locally. Next Steps:

- [Run your LLM App on AWS](/templates/llm-app/run-aws)
- Create a [git repository for your workspace](/how-to/git-repo)
- Read how to [manage the development application](/how-to/development-app)
- Read how to [format and validate your code](/how-to/format-and-validate)
- Read how to [add python libraries](/how-to/python-libraries)
- Chat with us on [discord](https://discord.gg/4MtYHHrgA8)
