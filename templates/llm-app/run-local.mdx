---
title: "Build an LLM App"
sidebarTitle: "Run locally"
---

The `llm-app` template gives us a production-ready LLM App built with [FastApi](https://fastapi.tiangolo.com/), [Streamlit](https://streamlit.io/) and [PgVector](https://github.com/pgvector/pgvector).

**By the end of this guide have your own LLM App built with:**

- **GPT-4** as the LLM
- **PostgreSQL** for knowledge and storage
- **Streamlit** and **FastApi** for serving
- **Docker** for running locally
- **AWS** for running in production

<Snippet file="setup.mdx" />

<Snippet file="create-llm-app-codebase.mdx" />

<Snippet file="set-openai-key.mdx" />

<Snippet file="serve-llm-app-streamlit.mdx" />

<Snippet file="chat-with-pdfs-rag.mdx" />

<Snippet file="llm-app-add-data.mdx" />

<Snippet file="llm-app-how-this-app-works.mdx" />

<Snippet file="serve-llm-app-fastapi.mdx" />

### View API Endpoints

- Open [localhost:8000/docs](http://localhost:8000/docs) to view the API Endpoints.
- Load the knowledge base using `/v1/pdf/conversation/load-knowledge-base`
- Test the `v1/pdf/conversation/chat` endpoint with

```json
{
  "message": "How do I make chicken curry?"
}
```

![rag-llm-app-fastapi-local](/images/rag-llm-app-fastapi-local.png)

## Build your LLM Product

Your LLM Api provides common endpoints you can use to build your LLM product. These routes are developed in collaboration with real AI Apps and are a great starting point to build on.

For example:

- Call the `/conversation/create` endpoint to create a new conversation for a user.

```json
{
  "user_name": "my-app-user-1",
  "conversation_type": "RAG"
}
```

- The response contains a `conversation_id` that can be used to build a chat interface by calling the `/conversation/chat` endpoint.

```json
{
  "message": "how do I make pasta",
  "stream": true,
  "conversation_id": "fc67f202-c2cb-4c3d-a640-704ab83d9460",
  "conversation_type": "RAG"
}
```

These routes are defined the `api/routes` folder and can be customized to your use case.

<Tip>
  Message us on [discord](https://discord.gg/4MtYHHrgA8) if you need help.
</Tip>

<Snippet file="llm-app-run-jupyter.mdx" />

<Snippet file="llm-app-delete-local-resources.mdx" />

## Next

Congratulations on running your LLM App locally. Next Steps:

- [Run your LLM App on AWS](/templates/llm-app/run-aws)
- Create a [git repository for your workspace](/how-to/git-repo)
- Read how to [manage the development application](/how-to/development-app)
- Read how to [format and validate your code](/how-to/format-and-validate)
- Read how to [add python libraries](/how-to/python-libraries)
- Chat with us on [discord](https://discord.gg/4MtYHHrgA8)
