---
title: Basics
---

**Assistant = LLM + memory + knowledge + tools**

- **Memory:** Stores chat history in a database.
- **Knowledge:** Stores business context in a vector database.
- **Tools:** Lets LLMs take actions like calling APIs, sending emails or querying a database.

Let's work through a few examples to get the hang of this.

## Install & Setup

<Steps>
  <Step title="Create a virtual environment">
    Open the `Terminal` and create a python virtual environment.

    <CodeGroup>

    ```bash Mac
    python3 -m venv ~/.venvs/aienv
    source ~/.venvs/aienv/bin/activate
    ```

    ```bash Windows
    python3 -m venv aienv
    aienv/scripts/activate
    ```

    </CodeGroup>

  </Step>
  <Step title="Install phidata">
    Install the latest version of `phidata`

    <CodeGroup>

    ```bash Mac
    pip install -U phidata
    ```

    ```bash Windows
    pip install -U phidata
    ```

    </CodeGroup>

  </Step>
</Steps>

## Assistants with Tools

Tools are **functions** that an Assistant can run to achieve tasks like searching the web, running SQL, sending an email, calling APIs.

### Web Search

Let's create an Assistant that can search the web using DuckDuckGo.

<Steps>
  <Step title="Create Web Search Assistant">
    Create a file `web_search.py`

    ```python web_search.py
    from phi.assistant import Assistant
    from phi.tools.duckduckgo import DuckDuckGo

    assistant = Assistant(tools=[DuckDuckGo()], show_tool_calls=True)
    assistant.print_response("Whats happening in France?", markdown=True)
    ```

  </Step>
  <Step title="Run the Assistant">
    Assistants use `OpenAI` by default. Set your `OPENAI_API_KEY`.

    <CodeGroup>

    ```bash Mac
    export OPENAI_API_KEY=sk-***
    ```

    ```bash Windows
    setx OPENAI_API_KEY sk-***
    ```

    </CodeGroup>

    Install `openai` & `duckduckgo`

    ```shell
    pip install openai duckduckgo-search
    ```

    Run the Assistant

    ```shell
    python web_search.py
    ```

  </Step>
</Steps>

### Data Analysis

Let's create an Assistant that can analyze data using DuckDb.

<Steps>
  <Step title="Create a DuckDbAssistant">
    Create a file `data_analyst.py`

    ```python data_analyst.py
    import json
    from phi.assistant.duckdb import DuckDbAssistant

    data_analyst = DuckDbAssistant(
        semantic_model=json.dumps({
            "tables": [
                {
                    "name": "movies",
                    "description": "Contains information about movies from IMDB.",
                    "path": "https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
                }
            ]
        }),
    )

    data_analyst.print_response("What is the average rating of movies? Show me the SQL.", markdown=True)
    ```

  </Step>
  <Step title="Run the Assistant">
    Install duckdb

    ```shell
    pip install duckdb
    ```

    Run the Assistant

    ```bash
    python data_analyst.py
    ```

  </Step>
</Steps>

### Python Assistant

Let's create an Assistant that can write and run python code to perform virtually any task.

<Steps>
  <Step title="Create a PythonAssistant">
    Create a file `python_assistant.py`

    ```python python_assistant.py
    from phi.assistant.python import PythonAssistant
    from phi.file.local.csv import CsvFile

    python_assistant = PythonAssistant(
        files=[
            CsvFile(
                path="https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
                description="Contains information about movies from IMDB.",
            )
        ],
        pip_install=True,
        show_tool_calls=True,
    )

    python_assistant.print_response("What is the average rating of movies?", markdown=True)
    ```

  </Step>
  <Step title="Run the Assistant">
    Install pandas

    ```
    pip install pandas
    ```

    Run the Assistant

    ```bash
    python python_assistant.py
    ```

  </Step>
</Steps>

<Note>
The `PythonAssistant` will create a file to achieve this task, please delete this file.
</Note>

## Assistants with Knowledge

To provide LLMs with business context, we store information in a vector database that the Assistant can search to improve its responses. This information provides extra context to language models so they respond in a context-aware manner.

Let's use `PgVector` as our vector store as it can also provide storage for our Assistants.

### Run PgVector

Run `PgVector` on `Docker` using the following steps:

<Steps>
  <Step title="Install Docker">
    Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) for running PgVector in a container.
  </Step>
  <Step title="Define PgVector as a resource">
    Create a file `resources.py` with the following contents

    ```python resources.py
    from phi.docker.app.postgres import PgVectorDb

    # -*- Run PgVector on port 5532 as port 5432 may be in use.
    vector_db = PgVectorDb(
        pg_user="ai",
        pg_password="ai",
        pg_database="ai",
        host_port=5532,
    )
    ```

  </Step>
  <Step title="Start PgVector">
    Start `resources.py` using:

    ```bash
    phi start resources.py
    ```

    **Press Enter** to confirm. You can verify container status on the docker dashboard.

  </Step>
</Steps>

<Note>
You may run PgVector any way you like, using `docker-compose` or `docker run` directly.
</Note>

### RAG Assistant

Retrieval Augmented Generation means **"stuffing the prompt with relevant information"** to improve LLM responses. This is a 2 step process:
1. Retrieve relevant information from a vector database.
2. Augment the prompt to provide context to the LLM.

Let's build a **PDF Assistant** that helps us with food recipes using RAG.

1. We'll load the knowledge base (vector db) with a recipe book PDF.
2. Our **PDF Assistant** will respond with the recipes from the knowledge base.

<Steps>
  <Step title="Install libraries">
    Install the required libraries using pip

    <CodeGroup>

    ```bash Mac
    pip install -U pgvector pypdf "psycopg[binary]" sqlalchemy
    ```

    ```bash Windows
    pip install -U pgvector pypdf "psycopg[binary]" sqlalchemy
    ```

    </CodeGroup>

  </Step>
  <Step title="Create a RAG Assistant">
    Create a file `rag_assistant.py` with the following contents

    ```python rag_assistant.py
    from phi.assistant import Assistant
    from phi.knowledge.pdf import PDFUrlKnowledgeBase
    from phi.vectordb.pgvector import PgVector2

    from resources import vector_db

    knowledge_base = PDFUrlKnowledgeBase(
        # Read PDF from this URL
        urls=["https://phi-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        # Store embeddings in the `ai.recipes` table
        vector_db=PgVector2(
            collection="recipes",
            db_url=vector_db.get_db_connection_local()
        ),
    )
    # Load the knowledge base
    knowledge_base.load(recreate=False)

    assistant = Assistant(
        knowledge_base=knowledge_base,
        # The add_references_to_prompt will update the prompt with references from the knowledge base.
        add_references_to_prompt=True,
    )
    assistant.print_response("How do I make pad thai?", markdown=True)
    ```

  </Step>
  <Step title="Run the Assistant">
    Run the Assistant (it takes a few seconds to load the knowledge base).

    <CodeGroup>

    ```bash Mac
    python rag_assistant.py
    ```

    ```bash Windows
    python rag_assistant.py
    ```

    </CodeGroup>

  </Step>
</Steps>

<Note>
The Assistant by default uses the `OpenAI` LLM and Embeddings.
</Note>

<Accordion title="How to use local PDFs" icon="file-pdf" iconType="duotone">
If you want to use local PDFs, use a `PDFKnowledgeBase` instead

```python assistant.py
from phi.knowledge.pdf import PDFKnowledgeBase

...
knowledge_base = PDFKnowledgeBase(
    path="data/pdfs",
    vector_db=PgVector2(
        collection="pdf_documents",
        db_url=vector_db.get_db_connection_local(),
    ),
)
...
```

</Accordion>

### Autonomous Assistant

With the RAG assistant above, the `add_references_to_prompt=True` always adds information from the knowledge base to the prompt, regardless of whether it is relevant to the question.

With Autonomous assistants, we let the LLM decide **if** it needs to access the knowledge base and what search parameters it needs to query the knowledge base.

Make the `Assistant` Autonomous by setting the `search_knowledge` and `read_chat_history` flags, giving it tools to search the knowledge base and chat history on demand.

<Steps>
  <Step title="Create an Autonomous Assistant">
    Create a file `auto_assistant.py` with the following contents

    ```python auto_assistant.py
    from phi.assistant import Assistant
    from phi.knowledge.pdf import PDFUrlKnowledgeBase
    from phi.vectordb.pgvector import PgVector2

    from resources import vector_db

    knowledge_base = PDFUrlKnowledgeBase(
        urls=["https://phi-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        vector_db=PgVector2(
            collection="recipes",
            db_url=vector_db.get_db_connection_local()
        ),
    )
    # Comment out as the knowledge base is already loaded.
    # knowledge_base.load(recreate=False)

    assistant = Assistant(
        knowledge_base=knowledge_base,
        # Show tool calls in the response
        show_tool_calls=True,
        # Enable the assistant to search the knowledge base
        search_knowledge=True,
        # Enable the assistant to read the chat history
        read_chat_history=True,
    )
    assistant.print_response("How do I make pad thai?", markdown=True)
    assistant.print_response("What was my last question?", markdown=True)
    ```

  </Step>
  <Step title="Run the assistant">
    Run the Assistant

    <CodeGroup>

    ```bash Mac
    python auto_assistant.py
    ```

    ```bash Windows
    python auto_assistant.py
    ```

    </CodeGroup>

    > Notice how it searches the knowledge base and chat history when needed

  </Step>
</Steps>

## Memory & Storage

Every `Assistant` comes with built-in memory but it only lasts while the session is active. To continue conversations across sessions, we need to store the chat history in a database like PostgreSQL.

We can easily add long-term memory to Assistants using a storage backend, for example:

```python storage.py
from phi.storage.assistant.postgres import PgAssistantStorage

from resources import vector_db

# Create a storage backend using the Postgres database
storage = PgAssistantStorage(
    table_name="assistant_storage",
    db_url=vector_db.get_db_connection_local(),
)

# Add storage to Assistant
assistant = Assistant(storage=storage)
```

## Assistant with Memory, Knowledge and Tools

Lets wrap up by building an Assistant that can read PDFs, store them in a knowledge base and continue conversations across sessions. Create a file `pdf_assistant.py`

```python pdf_assistant.py
import typer
from rich.prompt import Prompt
from typing import Optional, List
from phi.assistant import Assistant
from phi.storage.assistant.postgres import PgAssistantStorage
from phi.knowledge.pdf import PDFUrlKnowledgeBase
from phi.vectordb.pgvector import PgVector2

from resources import vector_db

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://phi-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector2(
        collection="recipes",
        db_url=vector_db.get_db_connection_local(),
    ),
)
# Comment out after first run
knowledge_base.load()

storage = PgAssistantStorage(
    table_name="pdf_assistant",
    db_url=vector_db.get_db_connection_local(),
)


def pdf_assistant(new: bool = False, user: str = "user"):
    run_id: Optional[str] = None

    if not new:
        existing_run_ids: List[str] = storage.get_all_run_ids(user)
        if len(existing_run_ids) > 0:
            run_id = existing_run_ids[0]

    assistant = Assistant(
        run_id=run_id,
        user_id=user,
        knowledge_base=knowledge_base,
        storage=storage,
        # Show tool calls in the response
        show_tool_calls=True,
        # Enable the assistant to search the knowledge base
        search_knowledge=True,
        # Enable the assistant to read the chat history
        read_chat_history=True,
    )
    if run_id is None:
        run_id = assistant.run_id
        print(f"Started Run: {run_id}\n")
    else:
        print(f"Continuing Run: {run_id}\n")

    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        assistant.print_response(message, markdown=True)


if __name__ == "__main__":
    typer.run(pdf_assistant)
```

**Run the Assistant**

<CodeGroup>

```bash Mac
python pdf_assistant.py
```

```bash Windows
python pdf_assistant.py
```

</CodeGroup>

**Ask Questions**

Now the assistant continues across sessions. Ask a question:

```
How do I make pad that?
```

Then message `bye` to exit, start the app again and ask:

```
What was my last message?
```

Run the `pdf_assistant.py` file with the `--new` flag to start a new run.

<CodeGroup>

```bash Mac
python pdf_assistant.py --new
```

```bash Windows
python pdf_assistant.py --new
```

</CodeGroup>

## Stop PgVector

Play around and then stop `PgVector` using `phi stop resources.py`

<CodeGroup>

```bash Mac
phi stop resources.py
```

```bash Windows
phi stop resources.py
```

</CodeGroup>

## Next

Congratulations on building an AI Assistant with memory, knowledge and tools. You can serve this Assistant in production using `FastApi`, `Streamlit` or `Django`. Read more about:
- [Assistants](/assistants/introduction)
- [Tools](/tools/introduction)
- [Knowledge](/knowledge/introduction)
- [Storage](/storage/introduction)

Or Checkout the following examples:
- [SQL Assistant](/examples/use-cases/sql)
- [Research Assistant](/examples/use-cases/research)

If you need help, reach out to us on [Discord](https://discord.com/invite/4MtYHHrgA8).