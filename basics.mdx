---
title: Basics
---

Let's learn the basics by building a mini AI App with a knowledge base, storage and memory.

## Conversations

With phidata, we build AI products using **Conversations**.

**Conversations** are a human-like interface to language models. We send the LLM a message and get a model-generated output as a response. Conversations have built-in **Memory**, **Knowledge**, **Storage** and **Tools**. Giving LLMs the ability to have long-term, knowledge-based conversations is the first step in our journey to AGI.

### Try it out

<Steps>
  <Step title="Create a virtual environment">
    Open the `Terminal` and create an `ai` directory with a python virtual environment.

    <CodeGroup>

    ```bash Mac
    mkdir ai && cd ai

    python3 -m venv aienv
    source aienv/bin/activate
    ```

    ```bash Windows
    mkdir ai; cd ai

    python3 -m venv aienv
    aienv/scripts/activate
    ```

    </CodeGroup>

  </Step>
  <Step title="Install libraries">
    Install `phidata` and `openai` using pip

    <CodeGroup>

    ```bash Mac
    pip install -U phidata openai
    ```

    ```bash Windows
    pip install -U phidata openai
    ```

    </CodeGroup>

  </Step>
  <Step title="Create a conversation">
    Create a file `conversation.py` with the following contents

    ```python conversation.py
    from phi.conversation import Conversation

    conversation = Conversation()

    # -*- Print a response
    conversation.print_response('Share a quick healthy breakfast recipe.')
    ```

  </Step>
  <Step title="Set your OpenAI key">

    Set the `OPENAI_API_KEY` environment variable to your OpenAI Key. You can get one [from OpenAI here](https://platform.openai.com/account/api-keys). Otherwise `phi` provides ready to use access to OpenAI models.

    <CodeGroup>

    ```bash Mac
    export OPENAI_API_KEY=sk-***
    ```

    ```bash Windows
    setx OPENAI_API_KEY sk-***
    ```

    </CodeGroup>

  </Step>
  <Step title="Run the conversation">
    Run the `conversation.py` file

    ```bash
    python conversation.py
    ```

  </Step>
</Steps>

## Agents

We can have Conversations with `Agents` designed for specific tasks.

### Python Engineer

The `PythonAgent` can perform virtually any task using python code.

<Steps>
  <Step title="Create a PythonAgent">
    Create a file `python_agent.py` and install pandas using `pip install pandas`

    ```python python_agent.py
    from phi.agent.python import PythonAgent
    from phi.file.local.csv import CsvFile

    python_agent = PythonAgent(
        files=[
            CsvFile(
                path="https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
                description="Contains information about movies from IMDB.",
            )
        ],
        pip_install=True,
        show_function_calls=True,
    )

    python_agent.print_response("What is the average rating of movies?")
    ```

  </Step>
  <Step title="Run the conversation">
    Run the `python_agent.py` file

    ```bash
    python python_agent.py
    ```

  </Step>
</Steps>

### Data Analyst

The `DuckDbAgent` can perform data analysis using SQL queries.

<Steps>
  <Step title="Create a DuckDbAgent">
    Create a file `data_analyst.py` and install duckdb using `pip install duckdb`

    ```python data_analyst.py
    import json
    from phi.agent.duckdb import DuckDbAgent

    duckdb_agent = DuckDbAgent(
        semantic_model=json.dumps({
            "tables": [
                {
                    "name": "movies",
                    "description": "Contains information about movies from IMDB.",
                    "path": "https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
                }
            ]
        }),
    )

    duckdb_agent.print_response("What is the average rating of movies? Show me the SQL.")
    ```

  </Step>
  <Step title="Run the conversation">
    Run the `data_analyst.py` file

    ```bash
    python data_analyst.py
    ```

  </Step>
</Steps>

## Knowledge Base

Knowledge Base is a database of information that the LLM can search to improve its responses. They are used to provide extra context to LLMs so they respond in a context-aware manner.

While any type of storage can act as a knowledge base, vector databases offer the best solution for retrieving relevant results from dense information quickly. Let's use `PgVector` as our vector store as it can also provide long-term storage to our Conversations.

### Run PgVector

Run `PgVector` on `Docker` using the following steps:

<Steps>
  <Step title="Install Docker">
    Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) for running PgVector in a container.
  </Step>
  <Step title="Define PgVector as a resource">
    Create a file `resources.py` with the following contents

    ```python resources.py
    from phi.docker.app.postgres import PgVectorDb
    from phi.docker.resources import DockerResources

    # -*- PgVector running on port 5432:5432
    vector_db = PgVectorDb(
        pg_user="llm",
        pg_password="llm",
        pg_database="llm",
        debug_mode=True,
    )

    # -*- DockerResources
    dev_docker_resources = DockerResources(apps=[vector_db])
    ```

  </Step>
  <Step title="Start PgVector">
    Start `resources.py` using:

    ```bash
    phi start resources.py
    ```

    **Press Enter** to confirm. Verify container status and view logs on the docker dashboard.

  </Step>
</Steps>

## RAG Conversation

Retrieval Augmented Generation means providing the LLM with additional information so it responds in a context-aware manner. When a user sends a message, we **"stuff the prompt"** with relevant information to improve response quality.

Let's test out RAG by building a **PDF Assistant** that helps us parse a recipe book.

1. We'll load our knowledge base with the PDF of a recipe book.
2. Our **PDF Assistant** will respond with the recipes from our knowledge base.

<Steps>
  <Step title="Install libraries">
    Install the required libraries using pip

    <CodeGroup>

    ```bash Mac
    pip install -U pgvector pypdf psycopg sqlalchemy
    ```

    ```bash Windows
    pip install -U pgvector pypdf psycopg sqlalchemy
    ```

    </CodeGroup>

  </Step>
  <Step title="Create a RAG Conversation">
    Create a file `rag_conversation.py` with the following contents

    ```python rag_conversation.py
    from phi.conversation import Conversation
    from phi.knowledge.pdf import PDFUrlKnowledgeBase
    from phi.vectordb.pgvector import PgVector

    from resources import vector_db

    # The PDFUrlKnowledgeBase reads PDFs from urls and loads
    # the `llm.recipes` table when`knowledge_base.load()` is called.
    knowledge_base = PDFUrlKnowledgeBase(
        urls=["https://www.family-action.org.uk/content/uploads/2019/07/meals-more-recipes.pdf"],
        vector_db=PgVector(
            collection="recipes",
            db_url=vector_db.get_db_connection_local(),
        ),
    )
    knowledge_base.load(recreate=False)

    conversation = Conversation(
        knowledge_base=knowledge_base,
        # The add_references_to_prompt flag searches the knowledge base
        # and updates the prompt sent to the LLM.
        add_references_to_prompt=True,
    )

    conversation.print_response('Share a quick recipe for a healthy breakfast.')
    conversation.print_response('How do I make chicken tikka salad?')
    ```

  </Step>
  <Step title="Run the conversation">
    Run the `rag_conversation.py` file. Give it a few minutes to load the knowledge base and see it respond with recipes from the recipe book.

    <CodeGroup>

    ```bash Mac
    python rag_conversation.py
    ```

    ```bash Windows
    python rag_conversation.py
    ```

    </CodeGroup>

  </Step>
</Steps>

<Accordion title="Use local PDFs">
If you want to use local PDFs, use a `PDFKnowledgeBase` instead

```python conversation.py
from phi.knowledge.pdf import PDFKnowledgeBase

...
knowledge_base = PDFKnowledgeBase(
    path="data/pdfs",
    vector_db=PgVector(
        collection="pdf_documents",
        db_url=vector_db.get_db_connection_local(),
    ),
)
...
```

</Accordion>

## Autonomous Conversation

In the RAG conversation above, the `add_references_to_prompt=True` was always adding information from the knowledge base to the prompt, regardless of whether it was helpful.

In Autonomous conversations, we let the LLM decide **if** it needs to access the knowledge base and what search parameters it needs to query the knowledge base.

Make the `Conversation` Autonomous by setting `function_calls=True` - which gives it the ability to call functions to achieve tasks.

<Steps>
  <Step title="Create an Autonomous Conversation">
    Create a file `auto_conversation.py` with the following contents

    ```python auto_conversation.py
    from phi.conversation import Conversation
    from phi.knowledge.pdf import PDFUrlKnowledgeBase
    from phi.vectordb.pgvector import PgVector

    from resources import vector_db

    knowledge_base = PDFUrlKnowledgeBase(
        urls=["https://www.family-action.org.uk/content/uploads/2019/07/meals-more-recipes.pdf"],
        vector_db=PgVector(
            collection="recipes",
            db_url=vector_db.get_db_connection_local(),
        ),
    )
    # Commented out as the knowledge base is already loaded.
    # knowledge_base.load(recreate=False)

    conversation = Conversation(
        knowledge_base=knowledge_base,
        # add_references_to_prompt=True,
        function_calls=True,
        show_function_calls=True,
    )

    conversation.print_response('How do I make chicken tikka salad?')
    conversation.print_response('What was my last question?')
    ```

  </Step>
  <Step title="Run the conversation">
    Run the `auto_conversation.py` file and notice how it searches the knowledge base and chat history when needed.

    <CodeGroup>

    ```bash Mac
    python auto_conversation.py
    ```

    ```bash Windows
    python auto_conversation.py
    ```

    </CodeGroup>

  </Step>
</Steps>

## Storage & Memory

Every `Conversation` comes with built-in memory but it only lasts while the session is active. To provide long-term memory we use a `Storage` backend like `PostgreSQL`.

<Steps>
  <Step title="Create a PDF Assistant with Knowledge and Storage">

    Create a file `pdf_assistant.py` with the following contents

```python pdf_assistant.py
import typer
from rich.prompt import Prompt
from typing import Optional, List

from phi.conversation import Conversation
from phi.storage.conversation.postgres import PgConversationStorage
from phi.knowledge.pdf import PDFUrlKnowledgeBase
from phi.vectordb.pgvector import PgVector

from resources import vector_db

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://www.family-action.org.uk/content/uploads/2019/07/meals-more-recipes.pdf"],
    vector_db=PgVector(
        collection="recipes",
        db_url=vector_db.get_db_connection_local(),
    ),
)

storage = PgConversationStorage(
    table_name="recipe_conversations",
    db_url=vector_db.get_db_connection_local(),
)


def pdf_assistant(new: bool = False, user: str = "user"):
    conversation_id: Optional[str] = None

    if not new:
        existing_conversation_ids: List[str] = storage.get_all_conversation_ids(user)
        if len(existing_conversation_ids) > 0:
            conversation_id = existing_conversation_ids[0]

    conversation = Conversation(
        user_name=user,
        id=conversation_id,
        knowledge_base=knowledge_base,
        storage=storage,
        # add_references_to_prompt=True,
        function_calls=True,
        show_function_calls=True,
    )
    if conversation_id is None:
        conversation_id = conversation.id
        print(f"Started Conversation: {conversation_id}\n")
    else:
        print(f"Continuing Conversation: {conversation_id}\n")

    # conversation.knowledge_base.load(recreate=False)
    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        conversation.print_response(message)


if __name__ == "__main__":
    typer.run(pdf_assistant)
```

  </Step>
  <Step title="Run the PDF Assistant">
    Run the `pdf_assistant.py` file.

    <CodeGroup>

    ```bash Mac
    python pdf_assistant.py
    ```

    ```bash Windows
    python pdf_assistant.py
    ```

    </CodeGroup>

  </Step>
  <Step title="Ask Questions">

    Now the conversation continues across sessions. Ask a question:

    ```
    How do I make chicken tikka salad?
    ```

    Then message `bye` to exit, start the app again and ask:

    ```
    What was my last message?
    ```

    Run the `conversation.py` file with the `--new` flag to start a new conversation.

    <CodeGroup>

    ```bash Mac
    python conversation.py --new
    ```

    ```bash Windows
    python conversation.py --new
    ```

    </CodeGroup>

  </Step>
</Steps>

## Stop PgVector

Play around and then stop `PgVector` using `phi stop resources.py`

<CodeGroup>

```bash Mac
phi stop resources.py
```

```bash Windows
phi stop resources.py
```

</CodeGroup>

## Next

Congratulations on building a mini AI App that can operate in RAG or Autonomous mode, with access to a knowledge base and storage. We can serve this App in production using an Api built with `FastApi`, front-end built with `Streamlit` or a Web App built with `Django`

Instead of wiring tools manually, phidata provides **pre-built templates** that you can run locally or on AWS with just 1 command. These templates are fully customizable and used by many teams in production. Checkout one of the examples below to start building

<Snippet file="examples.mdx" />
