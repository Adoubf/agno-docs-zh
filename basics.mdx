---
title: Basics
---

This guide walks through the basics of building an AI App. We'll learn by building a mini AI App with a knowledge base, storage and memory.

## Conversations

**Conversations** are a human-like interface to language models and the starting point for every AI App. We send the LLM a message and get a model-generated output as a response.

Conversations come with built-in **Memory**, **Knowledge**, **Storage** and access to **Tools**. Giving LLMs the ability to have long-term, knowledge-based **Conversations** is the first step in our journey to AGI. Using phidata, we can have conversations with AI specialists designed for specific tasks, for example:

- **PDF Assistants:** Answer questions using PDFs.
- **Python Engineers:** Perform tasks by writing and running python scripts.
- **Data Analysts:** Analyze data by writing and running SQL queries.
- **Stock Analysts:** Analyze stocks and research companies.

### Try it out

<Steps>
  <Step title="Create a virtual environment">
    Open the `Terminal` and create an `ai` directory with a python virtual environment.

    <CodeGroup>

    ```bash Mac
    mkdir ai && cd ai

    python3 -m venv aienv
    source aienv/bin/activate
    ```

    ```bash Windows
    mkdir ai; cd ai

    python3 -m venv aienv
    aienv/scripts/activate
    ```

    </CodeGroup>

  </Step>
  <Step title="Install libraries">
    Install `phidata` and `openai` using pip

    <CodeGroup>

    ```bash Mac
    pip install -U phidata openai
    ```

    ```bash Windows
    pip install -U phidata openai
    ```

    </CodeGroup>

  </Step>
  <Step title="Create a conversation">
    Create a file `conversation.py` with the following contents

    ```python conversation.py
    from phi.conversation import Conversation

    conversation = Conversation()

    # -*- Print a response
    conversation.print_response('Share a quick healthy breakfast recipe.')
    ```

  </Step>
  <Step title="Set your OpenAI key">

    Set the `OPENAI_API_KEY` environment variable to your OpenAI Key. You can get one [from OpenAI here](https://platform.openai.com/account/api-keys). Otherwise `phi` provides ready to use access to OpenAI models.

    <CodeGroup>

    ```bash Mac
    export OPENAI_API_KEY=sk-***
    ```

    ```bash Windows
    setx OPENAI_API_KEY sk-***
    ```

    </CodeGroup>

  </Step>
  <Step title="Run the conversation">
    Run the `conversation.py` file

    ```bash
    python conversation.py
    ```

  </Step>
</Steps>

## Agents

You can have a Conversation with an `Agent` designed for a specific task.

### Python Engineer

The `PythonAgent` can perform virtually any task using python code.

<Steps>
  <Step title="Create a PythonAgent">
    Create a file `python_agent.py` and install pandas using `pip install pandas`

    ```python python_agent.py
    from phi.agent.python import PythonAgent
    from phi.file.local.csv import CsvFile

    python_agent = PythonAgent(
        files=[
            CsvFile(
                path="https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
                description="Contains information about movies from IMDB.",
            )
        ],
        pip_install=True,
        show_function_calls=True,
    )

    python_agent.print_response("What is the average rating of movies?")
    ```

  </Step>
  <Step title="Run the conversation">
    Run the `python_agent.py` file

    ```bash
    python python_agent.py
    ```

  </Step>
</Steps>

### Data Analyst

The `DuckDbAgent` can perform data analysis using SQL queries.

<Steps>
  <Step title="Create a DuckDbAgent">
    Create a file `data_analyst.py` and install duckdb using `pip install duckdb`

    ```python data_analyst.py
    import json
    from phi.agent.duckdb import DuckDbAgent

    duckdb_agent = DuckDbAgent(
        semantic_model=json.dumps({
            "tables": [
                {
                    "name": "movies",
                    "description": "Contains information about movies from IMDB.",
                    "path": "https://phidata-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv",
                }
            ]
        }),
    )

    duckdb_agent.print_response("What is the average rating of movies? Show me the SQL.")
    ```

  </Step>
  <Step title="Run the conversation">
    Run the `data_analyst.py` file

    ```bash
    python data_analyst.py
    ```

  </Step>
</Steps>

## Knowledge Base

A Knowledge Base is a database of information that the LLM can search to improve its responses. They are used to provide additional context to LLMs and make them respond in a context-aware manner. This is known as Retrieval Augmented Generation (**RAG**).

While any type of storage can act as a knowledge base, vector databases offer the best solution for retrieving relevant results from dense information quickly. We'll use `PgVector` as our vector store because it can also provide long-term storage to our Conversations.

<Note>

While it would be easier to demo with an in-memory vector store, our goal is to stay as close to production-ready as possible.

</Note>

### Run PgVector

Run `PgVector` on `Docker` using the following steps:

<Steps>
  <Step title="Install Docker">
    Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) for running PgVector in a container.
  </Step>
  <Step title="Define PgVector as a resource">
    Create a file `resources.py` with the following contents

    ```python resources.py
    from phi.docker.app.postgres import PgVectorDb
    from phi.docker.resources import DockerResources

    # -*- PgVector running on port 5432:5432
    vector_db = PgVectorDb(
        pg_user="llm",
        pg_password="llm",
        pg_database="llm",
        debug_mode=True,
    )

    # -*- DockerResources
    dev_docker_resources = DockerResources(
        apps=[vector_db],
    )
    ```

  </Step>
  <Step title="Start PgVector">
    Start `resources.py` using:

    ```bash
    phi start resources.py
    ```

    **Press Enter** to confirm. Verify container status and view logs on the docker dashboard.

  </Step>
</Steps>

### Load Knowledge Base

Next, lets load our knowledge base using PDFs from the web.

<Steps>
  <Step title="Install libraries">
    Install the required libraries using pip

    <CodeGroup>

    ```bash Mac
    pip install -U pgvector pypdf psycopg sqlalchemy
    ```

    ```bash Windows
    pip install -U pgvector pypdf psycopg sqlalchemy
    ```

    </CodeGroup>

  </Step>
  <Step title="Update the Conversation">
    Update the `conversation.py` file to:

    ```python conversation.py
    from phi.conversation import Conversation
    from phi.knowledge.pdf import PDFUrlKnowledgeBase
    from phi.vectordb.pgvector import PgVector

    from resources import vector_db

    knowledge_base = PDFUrlKnowledgeBase(
        urls=["https://www.family-action.org.uk/content/uploads/2019/07/meals-more-recipes.pdf"],
        vector_db=PgVector(
            collection="recipes",
            db_url=vector_db.get_db_connection_local(),
        ),
    )
    knowledge_base.load(recreate=False)

    conversation = Conversation(
        knowledge_base=knowledge_base,
        add_references_to_prompt=True,
    )

    conversation.print_response('Share a quick recipe for a healthy breakfast.')
    conversation.print_response('How do I make chicken tikka salad?')
    ```

    <Note>

    The `PDFUrlKnowledgeBase` will load PDFs from the urls to the `llm.recipes` table.

    </Note>

  </Step>
  <Step title="Run the conversation">
    Run the `conversation.py` file and give it a few minutes to load the knowledge base.

    <CodeGroup>

    ```bash Mac
    python conversation.py
    ```

    ```bash Windows
    python conversation.py
    ```

    </CodeGroup>

  </Step>
</Steps>

<Accordion title="Read local PDFs">
If you want to read local PDFs, use a `PDFKnowledgeBase` instead

```python conversation.py
from phi.knowledge.pdf import PDFKnowledgeBase

...
knowledge_base = PDFKnowledgeBase(
    path="data/pdfs",
    vector_db=PgVector(
        collection="pdf_documents",
        db_url=vector_db.get_db_connection_local(),
    ),
)
...
```

</Accordion>

## Autonomous Conversations

In the `RAG` conversation above, the `add_references_to_prompt=True` was always adding information from the knowledge base to the prompt, regardless of whether it was helpful.

In Autonomous conversations, we let the LLM decide **if** it needs to access the knowledge base and what search parameters it needs to query the knowledge base.

Make the `Conversation` Autonomous by setting `function_calls=True` - which gives it the ability to call functions to achieve tasks.

<Steps>
  <Step title="Update the Conversation">

    Update the `conversation.py` file to:

    ```python conversation.py
    from phi.conversation import Conversation
    from phi.knowledge.pdf import PDFUrlKnowledgeBase
    from phi.vectordb.pgvector import PgVector

    from resources import vector_db

    knowledge_base = PDFUrlKnowledgeBase(
        urls=["https://www.family-action.org.uk/content/uploads/2019/07/meals-more-recipes.pdf"],
        vector_db=PgVector(
            collection="recipes",
            db_url=vector_db.get_db_connection_local(),
        ),
    )
    # knowledge_base.load(recreate=False)

    conversation = Conversation(
        knowledge_base=knowledge_base,
        # add_references_to_prompt=True,
        function_calls=True,
        show_function_calls=True,
    )

    conversation.print_response('How do I make chicken tikka salad?')
    conversation.print_response('What was my last question?')
    ```

    <Note>

    Comment out the `knowledge_base.load()` as it is already loaded.

    </Note>

  </Step>
  <Step title="Run the conversation">
    Run the `conversation.py` file and notice how it searches the knowledge base and chat history when needed.

    <CodeGroup>

    ```bash Mac
    python conversation.py
    ```

    ```bash Windows
    python conversation.py
    ```

    </CodeGroup>

  </Step>
</Steps>

## Storage & Memory

Every `Conversation` comes with built-in memory but it only lasts while the session is active. To provide memory across sessions, we use long-term `Storage` backed by a `PostgreSQL` table. Update the `conversation.py` file to:

```python conversation.py
import typer
from rich.prompt import Prompt
from typing import Optional, List

from phi.conversation import Conversation
from phi.storage.conversation.postgres import PgConversationStorage
from phi.knowledge.pdf import PDFUrlKnowledgeBase
from phi.vectordb.pgvector import PgVector

from resources import vector_db

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://www.family-action.org.uk/content/uploads/2019/07/meals-more-recipes.pdf"],
    vector_db=PgVector(
        collection="recipes",
        db_url=vector_db.get_db_connection_local(),
    ),
)

storage = PgConversationStorage(
    table_name="recipe_conversations",
    db_url=vector_db.get_db_connection_local(),
)


def llm_app(new: bool = False, user: str = "user"):
    conversation_id: Optional[str] = None

    if not new:
        existing_conversation_ids: List[str] = storage.get_all_conversation_ids(user)
        if len(existing_conversation_ids) > 0:
            conversation_id = existing_conversation_ids[0]

    conversation = Conversation(
        user_name=user,
        id=conversation_id,
        knowledge_base=knowledge_base,
        storage=storage,
        # add_references_to_prompt=True,
        function_calls=True,
        show_function_calls=True,
    )
    if conversation_id is None:
        conversation_id = conversation.id
        print(f"Started Conversation: {conversation_id}\n")
    else:
        print(f"Continuing Conversation: {conversation_id}\n")

    # conversation.knowledge_base.load(recreate=False)
    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        conversation.print_response(message)


if __name__ == "__main__":
    typer.run(llm_app)
```

Run the `conversation.py` file.

<CodeGroup>

```bash Mac
python conversation.py
```

```bash Windows
python conversation.py
```

</CodeGroup>

Now the conversation continues across sessions. Ask a question:

```
How do I make chicken tikka salad?
```

Then message `bye` to exit, start the app again and ask:

```
What was my last message?
```

Run the `conversation.py` file with the `--new` flag to start a new conversation.

<CodeGroup>

```bash Mac
python conversation.py --new
```

```bash Windows
python conversation.py --new
```

</CodeGroup>

## Stop Resources

Play around and then stop `PgVector` using `phi stop resources.py`

<CodeGroup>

```bash Mac
phi stop resources.py
```

```bash Windows
phi stop resources.py
```

</CodeGroup>

## Next

Congratulations on building a mini LLM App that can operate in RAG or Autonomous mode, with access to a knowledge base and storage. We can serve this App in production using an Api built with `FastApi`, front-end built with `Streamlit` or a Web App built with Django.

Instead of wiring tools manually, `phidata` provides pre-built templates that you can run locally or on AWS with just 1 command. These templates are fully customizable and used by many teams in production. Checkout one of the examples below to start building

<Snippet file="examples.mdx" />
