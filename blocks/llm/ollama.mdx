---
title: Ollama
---

[Ollama](https://ollama.com) is a fantastic tool for running LLMs locally. Install [ollama](https://ollama.com) and run a model using

<CodeGroup>

```bash run model
ollama run openhermes
```

```bash serve
ollama serve
```

</CodeGroup>

After you have the local model running, use the `Ollama` LLM to access them

## Usage

<CodeGroup>

```python assistant.py
from phi.assistant import Assistant
from phi.llm.ollama import Ollama

assistant = Assistant(
    llm=Ollama(model="llama2"),
    description="You help people with their health and fitness goals.",
)
assistant.print_response("Share a quick healthy breakfast recipe.", markdown=True)
```

</CodeGroup>

## Params

<Snippet file = "llm-ollama-reference.mdx"/>