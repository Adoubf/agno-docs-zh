---
title: Update Prompts
---

The `Conversation` class provides a user-friendly interface to LLMs, meaning behind the scenes it converts the message into `system` and `user` prompts.

Checkout the following examples on how to update prompts:

## Examples

**Update Prompts for RAG Conversation:**

```python rag_conversation.py
rag_conversation = Conversation(
    ...
    system_prompt="Talk to me like a pirate!",
    # The references variable is populated by setting add_references_to_prompt=True
    user_prompt_function=lambda message, references, **kwargs: f"""\
    Use the following information from the knowledge base if it helps.

    START OF KNOWLEDGE BASE
    ---
    {references}
    ---
    END OF KNOWLEDGE BASE

    Your task is to respond to the following message:
    USER: {message}
    ASSISTANT:
    """,
    # -*- Add references to the user prompt
    add_references_to_prompt=True,
    ...
)
```

**Update Prompts for Autonomous Conversation:**

```python auto_conversation.py
auto_conversation = Conversation(
    ...
    system_prompt="Talk to me like a pirate!",
    user_prompt_function=lambda message, **kwargs: f"""\
    Your task is to respond to the following message:
    USER: {message}
    ASSISTANT:
    """,
    ...
)
```

## Params

  <ResponseField name="system_prompt" type="str" default="None">
    If provided, sets the `System Prompt` sent to the LLM.
  </ResponseField>
  <ResponseField name="system_prompt_function" type="Callable[..., Optional[str]]" default="None">
    A function which returns the `System Prompt` sent to the LLM.

    Signature:
    ```python
      def system_prompt_function(conversation: Conversation) -> str:
          ...
    ```

  </ResponseField>
  <ResponseField name="use_default_system_prompt" type="str" default="True">
    If True, the conversation provides a default system prompt
  </ResponseField>
  <ResponseField name="user_prompt" type="str" default="None">
    If provided, sets the `User Prompt` sent to the LLM.
  </ResponseField>
  <ResponseField name="user_prompt_function" type="Callable[..., str]" default="None">
    A function which returns the `User Prompt` sent to the LLM.

    Signature:
    ```python
      def custom_user_prompt_function(
          conversation: Conversation,
          message: str,
          references: Optional[str] = None,
          chat_history: Optional[str] = None,
      ) -> str:
          ...
    ```
    This function is provided the `Conversation` object and the input `message` as arguments. It should return the `user_prompt` as a string.

    If `add_references_to_prompt` is True, then `references` are also provided as an argument.

    If `add_chat_history_to_prompt` is True, then `chat_history` is also provided as an argument.

  </ResponseField>
  <ResponseField name="use_default_user_prompt" type="str" default="True">
    If True, the conversation provides a default user prompt
  </ResponseField>
