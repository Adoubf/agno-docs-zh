---
title: Introduction
---

Conversations are a human-like interface to language models and the starting point for every AI App. We send the LLM a message and get a model-generated output as a response.

Conversations come with built-in **Memory**, **Knowledge**, **Storage** and access to **Tools**.

Giving LLMs the ability to have long-term, knowledge-based **Conversations** is the first step in our journey to AGI. A simple example looks like:

```python conversation.py
from phi.conversation import Conversation

conversation = Conversation()

# -*- Print a response
conversation.print_response('Share a quick healthy breakfast recipe.')

# -*- Get the response as a string
response = conversation.chat('Share a quick healthy breakfast recipe.', stream=False)

# -*- Get the response as a stream
response = ""
for delta in conversation.chat('Share a quick healthy breakfast recipe.'):
    response += delta
```

Under the hood, the message is converted to `System` and `User` prompts that are sent to the LLM. The default prompts can be easily customized to fit any use case.

Read more about:

- [RAG Conversations](/blocks/conversation/rag)
- [Autonomous Conversations](/blocks/conversation/auto)
- [Updating Prompts](/blocks/conversation/prompts)
