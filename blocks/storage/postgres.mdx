---
title: Postgres Conversation Storage
sidebarTitle: Postgres
---

Phidata supports using PostgreSQL as a storage backend for Conversations using the `PgConversationStorage` class.

## Usage

```python conversation.py
from phi.llm.openai import OpenAIChat
from phi.conversation import Conversation
from phi.storage.conversation.postgres import PgConversationStorage

conversation = Conversation(
    # -*- Add storage to the conversation
    storage = PgConversationStorage(
        # Table name: llm.conversations
        table_name="conversations",
        db_url=db_url,
    )
)
```

## Params

<ResponseField name="table_name" type="str">
  The name of the table to store conversations in.
</ResponseField>
<ResponseField name="schema" type="str" default="llm">
  The schema to store the table in.
</ResponseField>
<ResponseField name="db_url" type="str">
  The database URL to connect to.
</ResponseField>
<ResponseField name="db_engine" type="Engine">
  The database engine to use.
</ResponseField>

## Example

<Steps>
  <Step title="Create a virtual environment">
    Open the `Terminal` and create an `ai` directory with a python virtual environment.

    <CodeGroup>

    ```bash Mac
    mkdir ai && cd ai

    python3 -m venv aienv
    source aienv/bin/activate
    ```

    ```bash Windows
    mkdir ai; cd ai

    python3 -m venv aienv
    aienv/scripts/activate
    ```

    </CodeGroup>

  </Step>
  <Step title="Install libraries">
    Install libraries using pip

    <CodeGroup>

    ```bash Mac
    pip install -U phidata openai pgvector pypdf psycopg sqlalchemy
    ```

    ```bash Windows
    pip install -U phidata openai pgvector pypdf psycopg sqlalchemy
    ```

    </CodeGroup>

  </Step>
  <Step title="Start PgVector">
    Create a file `resources.py` with the following contents

```python resources.py
from phi.docker.app.postgres import PgVectorDb
from phi.docker.resources import DockerResources

# -*- PgVector running on port 5432:5432
vector_db = PgVectorDb(
    pg_user="llm",
    pg_password="llm",
    pg_database="llm",
    debug_mode=True,
)

# -*- DockerResources
dev_docker_resources = DockerResources(apps=[vector_db])
```

    Start `resources.py` using:

```bash
phi start resources.py
```

  </Step>
  <Step title="Create a conversation with storage">
    Create a file `conversation.py` with the following contents

```python conversation.py
import typer
from rich.prompt import Prompt
from typing import Optional, List

from phi.conversation import Conversation
from phi.storage.conversation.postgres import PgConversationStorage
from phi.knowledge.pdf import PDFUrlKnowledgeBase
from phi.vectordb.pgvector import PgVector

from resources import vector_db

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://www.family-action.org.uk/content/uploads/2019/07/meals-more-recipes.pdf"],
    vector_db=PgVector(
        collection="recipes",
        db_url=vector_db.get_db_connection_local(),
    ),
)

storage = PgConversationStorage(
    table_name="recipe_conversations",
    db_url=vector_db.get_db_connection_local(),
)


def llm_app(new: bool = False, user: str = "user"):
    conversation_id: Optional[str] = None

    if not new:
        existing_conversation_ids: List[str] = storage.get_all_conversation_ids(user)
        if len(existing_conversation_ids) > 0:
            conversation_id = existing_conversation_ids[0]

    conversation = Conversation(
        user_name=user,
        id=conversation_id,
        knowledge_base=knowledge_base,
        storage=storage,
        # add_references_to_prompt=True,
        function_calls=True,
        show_function_calls=True,
    )
    if conversation_id is None:
        conversation_id = conversation.id
        print(f"Started Conversation: {conversation_id}\n")
    else:
        print(f"Continuing Conversation: {conversation_id}\n")

    # conversation.knowledge_base.load(recreate=False)
    while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        conversation.print_response(message)


if __name__ == "__main__":
    typer.run(llm_app)
```

  </Step>
  <Step title="Run the conversation">
    Run the `conversation.py` file
```bash
python conversation.py
```

    Now the conversation continues across sessions. Ask a question:

    ```
    How do I make chicken tikka salad?
    ```

    Then message `bye` to exit, start the app again and ask:

    ```
    What was my last message?
    ```

  </Step>
  <Step title="Start a new conversation">
    Run the `conversation.py` file with the `--new` flag to start a new conversation.
```bash
python conversation.py --new
```
  </Step>
</Steps>

<Snippet file="discord.mdx" />
