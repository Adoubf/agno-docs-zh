---
title: vLLM
---
[vLLM](https://docs.vllm.ai/en/latest/) 是一个快速易用的LLM推理与服务库，专为高吞吐量和内存高效的LLM服务设计。

## 前提条件

安装vLLM并启动模型服务：

```bash 安装 vLLM
pip install vllm
```

```bash 启动 vLLM 服务器
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --enable-auto-tool-choice \
    --tool-call-parser hermes \
    --dtype float16 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.9
```

这将启动带有OpenAI兼容API的vLLM服务器。

<Note>
  默认vLLM服务器地址是 `http://localhost:8000/`
</Note>

## 示例

基础智能体
<CodeGroup>

```python agent.py
from agno.agent import Agent
from agno.models.vllm import vLLM

agent = Agent(
    model=vLLM(
        id="meta-llama/Llama-3.1-8B-Instruct", 
        base_url="http://localhost:8000/",
    ),
    markdown=True
)

agent.print_response("Share a 2 sentence horror story.")
```

</CodeGroup>

## 高级用法

### 工具集成

vLLM模型与Agno工具无缝协作：

```python with_tools.py
from agno.agent import Agent
from agno.models.vllm import vLLM
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=vLLM(id="meta-llama/Llama-3.1-8B-Instruct"),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)

agent.print_response("What's the latest news about AI?")
```

<Note> 查看更多示例请访问[这里](../examples/models/vllm)。</Note>

完整支持的模型列表请参阅[vLLM官方文档](https://docs.vllm.ai/en/latest/models/supported_models.html)。

## 参数

<Snippet file="model-vllm-params.mdx" />

`vLLM` 是[Model](/reference/models/model)类的子类，拥有相同的参数访问权限。