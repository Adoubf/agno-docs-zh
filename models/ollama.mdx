---
title: Ollama
description: 了解如何在 Agno 中使用 Ollama。
---
使用Ollama本地运行大语言模型

[Ollama](https://ollama.com) 是一个优秀的本地模型运行工具。

Ollama支持多种开源模型，模型库参见[这里](https://ollama.com/library)。

我们建议通过实验来找到最适合您用例的模型。以下是一些通用推荐：

- `llama3.3` 模型适用于大多数基础用例
- `qwen` 模型在工具使用场景表现突出
- `deepseek-r1` 模型具备强大的推理能力
- `phi4` 模型体积小巧但性能强大

## 模型设置

安装 [ollama](https://ollama.com) 后，通过以下命令运行模型：

```bash 运行模型
ollama run llama3.1
```

这将启动与模型的交互会话。

若要将模型下载用于Agno智能体，请使用：

```bash 拉取模型
ollama pull llama3.1
```

## 示例

本地获取模型后，使用 `Ollama` 模型类进行访问

<CodeGroup>

```python agent.py
from agno.agent import Agent, RunResponse
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="llama3.1"),
    markdown=True
)

# 在终端打印响应
agent.print_response("Share a 2 sentence horror story.")
```

</CodeGroup>

<Note> 查看更多示例请访问[这里](../examples/models/ollama)。</Note>

## 参数说明

<Snippet file="model-ollama-params.mdx" />

`Ollama` 是 [Model](/reference/models/model) 类的子类，拥有相同的参数访问权限。