---
title: Introduction
---

Vector databases enable us to store information as embeddings and search for the "results similar" to our input query using. These results are then provided to the LLM as context so it can respond in a context-aware manner using Retrieval Augmented Generation (**RAG**).

**Our goal is to search relevant information from a knowledge base quickly**, here's how vector databases are used with LLMs:

<Steps>
  <Step title="Chunk the information">
   Break down the knowledge into smaller chunks to ensure our search query matches only relevant results.
  </Step>
  <Step title="Load the knowledge base">
    Convert the chunks into embedding vectors and store them in a vector database.
  </Step>
  <Step title="Search the knowledge base">
    When the user sends a message, we convert the input message into an embedding and "search" for nearest neighbors in the vector database.
  </Step>
</Steps>

The following VectorDb are supported:

- [PgVector](/vectordb/pgvector)
- [SingleStore](/vectordb/singlestore)
