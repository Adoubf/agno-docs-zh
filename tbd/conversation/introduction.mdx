---
title: Introduction
---

Conversations are a human-like interface to language models and the starting point for every AI App. We send the LLM a message and get a model-generated output as a response.

Conversations come with built-in **Memory**, **Knowledge**, **Storage** and access to **Tools**.

Giving LLMs the ability to have long-term, knowledge-based **Conversations** is the first step in our journey to AGI. A simple example looks like:

```python conversation.py
from phi.conversation import Conversation

conversation = Conversation()

# -*- Print a response
conversation.print_response('Share a quick healthy breakfast recipe.')

# -*- Get the response as a string
response = conversation.run('Share a quick healthy breakfast recipe.', stream=False)

# -*- Get the response as a stream
response = ""
for delta in conversation.run('Share a quick healthy breakfast recipe.'):
    response += delta
```

Under the hood, the message is converted into prompts that are sent to the LLM. The default prompts can be customized to fit any use case.

## Example

<Snippet file="run-conversation-steps.mdx" />

## More Information

Read more about:

- [RAG Conversations](/blocks/conversation/rag)
- [Autonomous Conversations](/blocks/conversation/auto)
- [Updating Prompts](/blocks/conversation/prompts)
- [Memory](/blocks/conversation/memory)
- [Tools](/blocks/conversation/tools)
- [Generating Pydantic models as output](/blocks/conversation/pydantic-output)
- [Tasks](/blocks/conversation/tasks)
- [Delegating work to Assistants](/blocks/conversation/assistant)
