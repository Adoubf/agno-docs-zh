---
title: 简单智能体评估
description: 了解如何从三个关键维度评估您的Agno智能体与团队——准确性（使用LLM-as-a-judge方法）、性能（运行时与内存占用）以及可靠性（工具调用能力）。
sidebarTitle: 概述
---
**评估**（Evals）是用于测试智能体和团队表现的单元测试，合理运用它们来测量并提升性能。Agno为评估智能体提供了三个维度：
- **准确性**：智能体的响应有多完整/正确/精确（以LLM作为评判标准）
- **性能**：智能体响应速度如何？内存占用情况怎样？
- **可靠性**：智能体是否调用了预期工具？

## 准确性

准确性评估使用输入/输出配对来衡量智能体及团队表现与黄金标准答案的差距。通过更大规模的模型为智能体响应打分（以LLM作为评判者）。

#### 示例

本例中，`AccuracyEval` 将使用输入运行智能体，随后采用更大模型（`o4-mini`）根据给定准则对智能体响应进行评分。

```python calculate_accuracy.py
from typing import Optional
from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(model=OpenAIChat(id="gpt-4o"), tools=[CalculatorTools(enable_all=True)]),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

<Frame>
  <img
    height="200"
    src="/images/accuracy-eval-result.png"
    style={{ borderRadius: '8px' }}
  />
</Frame>

您也可以直接对已有输出运行 `AccuracyEval`（无需启动智能体）。

```python accuracy_eval_with_output.py
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    num_iterations=1,
)
result_with_given_answer: Optional[AccuracyResult] = evaluation.run_with_output(
    output="2500", print_results=True
)
assert result_with_given_answer is not None and result_with_given_answer.avg_score >= 8
```

## 性能

性能评估测量智能体或团队的延迟时间及内存占用情况。

<Note>
虽然延迟主要受模型API响应时间影响，但仍需保持性能优先意识，追踪智能体或团队在不同组件状态下的表现。例如：了解启用/禁用存储、记忆功能时的平均延迟，或测试新提示词、新模型时的表现差异至关重要。
</Note>

#### 示例

```python storage_performance.py
"""Run `pip install openai agno` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.eval.perf import PerfEval

def simple_response():
    agent = Agent(model=OpenAIChat(id='gpt-4o-mini'), system_message='Be concise, reply with one sentence.', add_history_to_messages=True)
    response_1 = agent.run('What is the capital of France?')
    print(response_1.content)
    response_2 = agent.run('How many people live there?')
    print(response_2.content)
    return response_2.content


simple_response_perf = PerfEval(func=simple_response, num_iterations=1, warmup_runs=0)

if __name__ == "__main__":
    simple_response_perf.run(print_results=True)
```

## 可靠性

如何判定智能体或团队是否可靠？

- 是否调用预期工具？
- 能否优雅处理错误？
- 是否遵守模型API的速率限制？

#### 示例

首要检查项是确认智能体调用预期工具。示例如下：

```python reliability.py
from typing import Optional

from agno.agent import Agent
from agno.eval.reliability import ReliabilityEval, ReliabilityResult
from agno.tools.calculator import CalculatorTools
from agno.models.openai import OpenAIChat
from agno.run.response import RunResponse


def multiply_and_exponentiate():

    agent=Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        tools=[CalculatorTools(add=True, multiply=True, exponentiate=True)],
    )
    response: RunResponse = agent.run("What is 10*5 then to the power of 2? do it step by step")
    evaluation = ReliabilityEval(
        agent_response=response,
        expected_tool_calls=["multiply", "exponentiate"],
    )
    result: Optional[ReliabilityResult] = evaluation.run(print_results=True)
    result.assert_passed()


if __name__ == "__main__":
    multiply_and_exponentiate()
```

<Note>
可靠性评估当前处于 `beta` 阶段。
</Note>