---
title: Ollama
---

[Ollama](https://ollama.com) is a fantastic tool for running LLMs locally. Install [ollama](https://ollama.com) and run a model using

<CodeGroup>

```bash run model
ollama run llama2
```

```bash serve
ollama serve
```

</CodeGroup>

After you have the local model running, use the `Ollama` LLM to access them

## Usage

<CodeGroup>

```python assistant.py
from phi.assistant import Assistant
from phi.llm.ollama import Ollama

assistant = Assistant(
    llm=Ollama(model="llama2"),
    description="You help people with their health and fitness goals.",
)
assistant.print_response("Share a quick healthy breakfast recipe.", markdown=True)
```

</CodeGroup>

## Params

<ResponseField name="model" type="str">
  Model name
</ResponseField>
<ResponseField name="host" type="str">
  Host url
</ResponseField>
<ResponseField name="format" type="str" default="">
  Response format, `""` or `"json"`
</ResponseField>
<ResponseField name="timeout" type="Any" default="None">
  Timeout for requests
</ResponseField>
<ResponseField name="options" type="Dict[str, Any]" default="None">
  Dictionary of options to send with the request, example: `{temperature: 0.1, stop: ['\n']}`
</ResponseField>
<ResponseField name="keep_alive" type="Union[float, str]" default="None">
</ResponseField>
<ResponseField name="client_kwargs" type="Dict[str, Any]" default="None">
  Additional `{key: value}` dict sent when initalizing the `Ollama()` client.
</ResponseField>
<ResponseField name="ollama_client" type="ollama.Client()" default="None">
  Provide your own `ollama.Client()`
</ResponseField>

<Snippet file="message-us-discord.mdx" />
