---
title: "Build an Autonomous AI App"
sidebarTitle: "Run locally"
---

Let's build an Autonomous AI App powered by GPT-4. We'll use PgVector for knowledge and storage and serve the app using Streamlit and FastApi.

## What is Autonomous mode?

Autonomous mode is when the LLM has access to a set of functions and it can run those functions to achieve tasks. Here's an example:

- With RAG, a user asks the LLM a question and we always add information to the prompt, regardless of whether it is helpful. This not only confuses the LLM and leads to quality issues, it uses extra tokens and increases latency and cost.
- With Autonomous mode, the LLM decides **if** it needs to access the knowledge base and what search parameters it needs to query the knowledge base.
- We can also provide functions to:
  - Get chat history when needed (further saving tokens).
  - Add content to the knowledge base (increasing its knowledge).
  - Complete tasks like send emails, create PRs, query data (baby steps towards AGI).

**By the end of this guide you'll have your own Autonomous AI App.**

<Snippet file="setup.mdx" />

<Tip>

If you already have an `ai-app` created, skip the next step.

</Tip>

<Snippet file="create-ai-app-codebase.mdx" />

<Snippet file="set-openai-key.mdx" />

<Snippet file="serve-ai-app-streamlit.mdx" />

### PDF Assistant

- Open [localhost:8501](http://localhost:8501) to view streamlit apps that you can customize and make your own.
- Click on **PDF Assistant** in the sidebar
- Enter a username and wait for the knowledge base to load.
- Choose the `Autonomous` Assistant type.
- Ask "How do I make chicken curry?"
- Notice how the LLM first searches the knowledge base. It also uses the appropriate search query, which is very useful when the user sends a long 3-4 sentence message and the usual RAG approach fails to retrieve the correct information.
- Upload PDFs and ask questions

![Chat with pdf](/images/ai-app-pdf-assistant-local.png)

<Snippet file="ai-app-add-data.mdx" />

## How this App works

The streamlit apps are defined in the `app` folder and the `Assistants` powering these apps are defined in the `ai/assistants` folder.

Checkout the `ai/assistants/pdf_auto.py` file for the Autonomous PDF Assistant.

## Add your own tools

Its super easy and even recommended to add your own tools to your autonomous app.

Read how to add [tools](/blocks/assistant/tools) to Assistants.

<Snippet file="serve-ai-app-fastapi.mdx" />

### View API Endpoints

- Open [localhost:8000/docs](http://localhost:8000/docs) to view the API Endpoints.
- Load the knowledge base using `/v1/assitants/load-knowledge-base`
- Test the `v1/assitants/chat` endpoint with

```json
{
  "message": "How do I make chicken curry?",
  "assistant": "AUTO_PDF"
}
```

![Local API Endpoints](/images/ai-app-api-endpoints-local.png)

<Snippet file="ai-app-build-your-ai-product.mdx" />

<Snippet file="ai-app-run-jupyter.mdx" />

<Snippet file="ai-app-delete-local-resources.mdx" />

## Next

Congratulations on running your AI App locally. Next Steps:

- [Run your AI App on AWS](/examples/auto/run-aws)
- Read how to [update workspace settings](/how-to/workspace-settings)
- Read how to [create a git repository for your workspace](/how-to/git-repo)
- Read how to [manage the development application](/how-to/development-app)
- Read how to [format and validate your code](/how-to/format-and-validate)
- Read how to [add python libraries](/how-to/python-libraries)
- Chat with us on [discord](https://discord.gg/4MtYHHrgA8)
