---
title: "Build a Multimodal AI App"
sidebarTitle: "Run locally"
---

Let's build a Multimodal AI App powered by GPT-4V. We'll use PgVector for knowledge and storage and serve the app using Streamlit.

## What is Multimodal?

Multimodal means the LLM can work with images along with text. Historically, language model systems have been limited to text but **GPT-4V** can also understand images (and with some workarounds - videos too). This unlocks a powerful set of new use-cases for LLMs.

**By the end of this guide have your own Multimodal LLM App.**

<Snippet file="setup.mdx" />

<Tip>

If you already have an `ai-app` created, skip the next step.

</Tip>

<Snippet file="create-ai-app-codebase.mdx" />

<Snippet file="set-openai-key.mdx" />

<Snippet file="serve-ai-app-streamlit.mdx" />

### Image Assistant

- Open [localhost:8501](http://localhost:8501) to view streamlit apps that you can customize and make your own.
- Click on **Image Assistant** in the sidebar.
- Enter a username.
- Upload an image of your choice.
- Click on `Generate Caption`, `Describe Image`, `Identify Brands`, `Identify Items` for examples.

![Image assistant](/images/ai-app-image-assistant-local.png)

<Snippet file="ai-app-how-this-app-works.mdx" />

## Serve your App using FastApi

<Note>

We are working on a set of common endpoints for the Multimodal LLM App.

</Note>

<Snippet file="ai-app-run-jupyter.mdx" />

<Snippet file="ai-app-delete-local-resources.mdx" />

## Next

Congratulations on running your AI App locally. Next Steps:

- [Run your AI App on AWS](/examples/multimodal/run-aws)
- Read how to [update workspace settings](/how-to/workspace-settings)
- Read how to [create a git repository for your workspace](/how-to/git-repo)
- Read how to [manage the development application](/how-to/development-app)
- Read how to [format and validate your code](/how-to/format-and-validate)
- Read how to [add python libraries](/how-to/python-libraries)
- Chat with us on [discord](https://discord.gg/4MtYHHrgA8)
