---
title: "Build a LLM API"
sidebarTitle: "Run locally"
---

Almost every production AI application is built using a front-end framework like [next.js](https://nextjs.org) combined with a RestAPI and database, a task where [FastApi](https://fastapi.tiangolo.com/) & [PostgreSQL](https://www.postgresql.org/) shine.

In this guide, let's build a RestApi for serving LLM conversations. We'll use:

- **GPT-4** as the LLM
- **FastApi** as the API
- **PostgreSQL** as the database providing knowledge and storage
- **Docker** for running locally
- **AWS** for running in production

<Note>

Phidata's own RestAPI serving millions of requests is built using this.

</Note>

<Snippet file="setup.mdx" />

## Create your codebase

Create your codebase using the `api-app` template pre-configured with [FastApi](https://fastapi.tiangolo.com/) and [PostgreSQL](https://www.postgresql.org/).

<CodeGroup>

```bash Mac
phi ws create -t api-app -n llm-api
```

```bash Windows
phi ws create -t api-app -n llm-api
```

</CodeGroup>

This will create a folder `llm-api` with the following structure:

```bash
llm-api                       # root directory for your llm-api
├── api                     # directory for FastApi routes
├── db                      # directory for database components
├── llm                     # directory for LLM components
    ├── conversations       # LLM conversations
    ├── knowledge_base.py   # LLM knowledge base
    └── storage.py          # LLM storage
├── Dockerfile              # Dockerfile for the application
├── pyproject.toml          # python project definition
├── requirements.txt        # python dependencies generated by pyproject.toml
├── scripts                 # directory for helper scripts
├── tests                   # directory for unit tests
├── utils                   # directory for shared utilities
└── workspace               # phidata workspace directory
    ├── dev_resources.py    # dev resources running locally
    ├── prd_resources.py    # production resources running on AWS
    ├── secrets             # directory for storing secrets
    └── settings.py         # phidata workspace settings
```

<Snippet file="set-openai-key.mdx" />

## Local LLM api & database

[FastApi](https://fastapi.tiangolo.com/) is an exceptional framework for building RestApis. Its fast, well-designed and loved by everyone using it. Your codebase comes pre-configured with [FastApi](https://fastapi.tiangolo.com/) and [PostgreSQL](https://www.postgresql.org/), along with some sample routes. Start your workspace using:

<CodeGroup>

```bash terminal
phi ws up
```

```bash shorthand
phi ws up dev:docker
```

</CodeGroup>

**Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.

### Sample API Endpoints

- Open [localhost:8000/docs](http://localhost:8000/docs) to view sample API Endpoints.
- Load the knowledge base using `/v1/pdf/conversation/load-knowledge-base`
- Test the `v1/pdf/conversation/chat` endpoint with

```json
{
  "message": "How do I make chicken curry?"
}
```

![api-app-fastapi-local](/images/api-app-fastapi-local.png)

<Snippet file="api-app-add-data.mdx" />

<Snippet file="api-app-build-ai-product.mdx" />

<Snippet file="stop-local-workspace.mdx" />

## Next

Congratulations on running your LLM API locally. Next Steps:

- [Run your LLM API on AWS](/examples/api/llm-api/run-aws)
- Create a [git repository for your workspace](/how-to/git-repo)
- Read how to [manage the development application](/how-to/development-app)
- Read how to [format and validate your code](/how-to/format-and-validate)
- Read how to [add python libraries](/how-to/python-libraries)
- Chat with us on [discord](https://discord.gg/4MtYHHrgA8)
