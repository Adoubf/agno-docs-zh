| 参数                | 类型                          | 默认值                      | 描述                                                                                   |
| ------------------------ | ----------------------------- | ---------------------------- | --------------------------------------------------------------------------------------------- |
| `id`                     | `str`                         | **必填**                 | 指定使用的模型ID（例如 `"Qwen/Qwen2.5-7B-Instruct"`）。 |
| `name`                   | `str`                         | `"vLLM"`                     | 该模型实例的名称。                                                                   |
| `provider`               | `str`                         | `"vLLM"`                     | 供应商名称。                                                                                |
| `api_key`                | `Optional[str]`               | `"EMPTY"`                    | API密钥（为兼容OpenAI规范发送；通常无需配置）。                              |
| `base_url`               | `str`                         | `"http://localhost:8000/v1/"` | vLLM服务器的URL（兼容OpenAI的端点）。                                          |
| `max_tokens`             | `Optional[int]`               | `None`                       | 生成的最大token数。                                                    |
| `temperature`            | `float`                       | `0.7`                        | 采样温度。                                                                          |
| `top_p`                  | `float`                       | `0.8`                        | 核心采样概率。                                                                   |
| `top_k`                  | `Optional[int]`               | `None`                       | 限制采样到top-K个token。                                                         |
| `frequency_penalty`      | `Optional[float]`             | `None`                       | 根据已生成文本中的词频惩罚新token。                           |
| `presence_penalty`       | `float`                       | `1.5`                        | 重复惩罚系数。                                                                            |
| `stop`                   | `Optional[Union[str, List[str]]]` | `None`                   | 最多4个停止序列，API将在生成这些token后停止输出。                        |
| `seed`                   | `Optional[int]`               | `None`                       | 确定性采样的随机种子。                                                           |
| `request_params`         | `Optional[Dict[str, Any]]`    | `None`                       | 合并到请求中的额外关键字参数。                                                |
| `client_params`          | `Optional[Dict[str, Any]]`    | `None`                       | 传递给客户端的附加参数。                                                 |
| `timeout`                | `Optional[float]`             | `None`                       | HTTP请求超时时间。                                                                 |
| `max_retries`            | `Optional[int]`               | `None`                       | 最大请求重试次数。                                                            |
| `enable_thinking`        | `Optional[bool]`              | `None`                       | 启用vLLM"思考"模式（在`chat_template_kwargs`中传递`enable_thinking`）。            |

<Note>
`vLLM` 同时支持[OpenAI](/reference/models/openai)的参数。
</Note>