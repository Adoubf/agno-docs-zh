<ResponseField name="llm" type="LLM" default="OpenAIChat()" required>
  LLM to use for this Assistant
</ResponseField>
<ResponseField name="introduction" type="str">
 Assistant introduction. This is added to the chat history when a run is started.
</ResponseField>
<ResponseField name="name" type="str">
 Assistant name
</ResponseField>
<ResponseField name="assistant_data" type="Dict[str, Any]">
 Metadata associated with this assistant
</ResponseField>

<ResponseField name="run_id" type="str">
 Run UUID (autogenerated if not set)
</ResponseField>
<ResponseField name="run_name" type="str">
 Run name
</ResponseField>
<ResponseField name="run_data" type="Dict[str, Any]">
 Metadata associated with this run
</ResponseField>

<ResponseField name="user_id" type="str">
  ID of the user participating in this run
</ResponseField>
<ResponseField name="user_data" type="Dict[str, Any]">
  Metadata associated the user participating in this run
</ResponseField>

<ResponseField name="memory" type="AssistantMemory" default="AssistantMemory()" required>
  Assistant Memory
</ResponseField>
<ResponseField name="add_chat_history_to_messages" type="bool" default="False">
  Add chat history to the messages sent to the LLM.
</ResponseField>
<ResponseField name="add_chat_history_to_prompt" type="bool" default="False">
  Add chat history to the prompt sent to the LLM.
</ResponseField>
<ResponseField name="num_history_messages" type="int" default="6">
  Number of previous messages to add to prompt or messages sent to the LLM.
</ResponseField>

<ResponseField name="knowledge_base" type="AssistantKnowledge">
  Assistant Knowledge Base
</ResponseField>
<ResponseField name="add_references_to_prompt" type="bool" default="False">
  Enable RAG by adding references from the knowledge base to the prompt.
</ResponseField>

<ResponseField name="storage" type="AssistantStorage" required>
  Assistant Storage
</ResponseField>
<ResponseField name="db_row" type="AssistantRun" required>
  AssistantRun from the database: DO NOT SET MANUALLY
</ResponseField>

<ResponseField name="tools" type="List[Union[Tool, ToolRegistry, Callable, Dict, Function]]">
 A list of tools provided to the LLM. Tools are functions the model may generate JSON inputs for.
 If you provide a dict, it is not called by the model.
</ResponseField>
<ResponseField name="use_tools" type="bool" default="False">
 Allow the assistant to use tools
</ResponseField>
<ResponseField name="show_tool_calls" type="bool" default="False">
 Show tool calls in LLM messages.
</ResponseField>
<ResponseField name="tool_call_limit" type="int">
 Maximum number of tool calls allowed.
</ResponseField>
<ResponseField name="tool_choice" type="Union[str, Dict[str, Any]]">
 Controls which (if any) tool is called by the model.
 "none" means the model will not call a tool and instead generates a message.
 "auto" means the model can pick between generating a message or calling a tool.
 Specifying a particular function via `{"type: "function", "function": {"name": "my_function"}}`
 forces the model to call that tool.
 "none" is the default when no tools are present. "auto" is the default if tools are present.
</ResponseField>
<ResponseField name="update_knowledge_base" type="bool" default="False">
 If use_tools is True and update_knowledge_base is True,
 then a tool is added that allows the LLM to update the knowledge base.
</ResponseField>
<ResponseField name="read_tool_call_history" type="bool" default="False">
 If use_tools is True and read_tool_call_history is True,
 then a tool is added that allows the LLM to get the tool call history.
</ResponseField>

<ResponseField name="system_prompt" type="str">
  Provide the system prompt as a string
</ResponseField>
<ResponseField name="system_prompt_function" type="Callable[..., Optional[str]]">
  Provide the system prompt as a function. This function is provided the "Assistant object" as an argument
  and should return the system_prompt as a string.

Signature:

```python
def system_prompt_function(assistant: Assistant) -> str:
    ...
```

</ResponseField>
<ResponseField name="build_default_system_prompt" type="Callable[..., Optional[str]]">
  If True, build a default system prompt using instructions and extra_instructions
</ResponseField>
<ResponseField name="description" type="str">
  Assistant description for the default system prompt
</ResponseField>
<ResponseField name="instructions" type="List[str]">
  List of instructions for the default system prompt
</ResponseField>
<ResponseField name="extra_instructions" type="bool" default="True">
  List of extra_instructions for the default system prompt
  Use these when you want to use the default prompt but also add some extra instructions
</ResponseField>
<ResponseField name="add_datetime_to_instructions" type="bool" default="False">
  If True, add the current datetime to the prompt to give the assistant a sense of time
  This allows for relative times like "tomorrow" to be used in the prompt
</ResponseField>
<ResponseField name="markdown" type="bool" default="True">
  If markdown=true, formats the output using markdown
</ResponseField>

<ResponseField name="user_prompt" type="Union[List[Dict], str]">
  Provides the user prompt as a string. Note: this will ignore the input message provided to the run function
</ResponseField>
<ResponseField name="user_prompt_function" type="Callable[..., str]">
  Provides the user prompt as a function. This function is provided the "Assistant object" and the "Input message" as arguments
  and should return the user_prompt as a Union[List[Dict], str].
  If `add_references_to_prompt` is True, then `references` are also provided as an argument.
  If `add_chat_history_to_prompt` is True, then `chat_history` is also provided as an argument.

Signature:

```python
def custom_user_prompt_function(
    assistant: Assistant,
    message: Union[List[Dict], str],
    references: Optional[str] = None,
    chat_history: Optional[str] = None,
) -> Union[List[Dict], str]:
    ...
```

</ResponseField>
<ResponseField name="build_default_user_prompt" type="bool" default="True">
  If True, build a default user prompt using references and chat history
</ResponseField>
<ResponseField name="references_function" type="Callable[..., Optional[str]]">
Function to build references for the default `user_prompt`. This function, if provided, is called when `add_references_to_prompt` is True

Signature:

```python
def references(assistant: Assistant, query: str) -> Optional[str]:
    ...
```

</ResponseField>
<ResponseField name="chat_history_function" type="Callable[..., Optional[str]]">
Function to build the chat_history for the default `user_prompt`. This function, if provided, is called when `add_chat_history_to_prompt` is True

Signature:

```python
def chat_history(assistant: Assistant) -> str:
    ...
```

</ResponseField>

<ResponseField name="output_model" type="Union[str, List, Type[BaseModel]]">
  Provide an output model for the responses
</ResponseField>
<ResponseField name="parse_output" type="bool" default="True">
  If True, the output is converted into the output_model (pydantic model or json
  dict)
</ResponseField>
<ResponseField name="output" type="Any">
  Final LLM response i.e. the final output of this assistant
</ResponseField>

<ResponseField name="tasks" type="List">
  Tasks allow the Assistant to generate a response using a list of tasks If
  tasks is None or empty, a single default LLM task is created for this
  assistant
</ResponseField>
<ResponseField name="task_data" type="Dict[str, Any]">
  Metadata associated with the assistant tasks
</ResponseField>

<ResponseField name="debug_mode" type="bool" default="False">
  If True, show debug logs
</ResponseField>
<ResponseField name="monitoring" type="bool" default="False">
  If True, logs Assistant runs on phidata.com
</ResponseField>
