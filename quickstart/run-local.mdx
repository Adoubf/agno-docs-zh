---
title: "Build a RAG LLM App"
sidebarTitle: "Run locally"
---

Let's build a RAG LLM App powered by GPT-4. We'll use PgVector for Knowledge Base and Storage and serve the app using Streamlit and FastApi.

## What is RAG?

Retrieval Augmented Generation means providing the LLM with additional information along with the question to improve its responses. Here's how it works:

- Without RAG, a user asks the LLM a question and the LLM responds with information from its training data.
- With RAG, a user asks the LLM a question and we "stuff the prompt" with relevant information from a knowledge base. The LLM then responds in a context-aware manner.
- Using RAG is like taking an open-book test.

**By the end of this guide you'll be up and running with your own RAG LLM App.**

<Snippet file="setup.mdx" />

## Create your codebase

Create your codebase using the `llm-app` template pre-configured with [FastApi](https://fastapi.tiangolo.com/), [Streamlit](https://streamlit.io/) and [PgVector](https://github.com/pgvector/pgvector).

<CodeGroup>

```bash Mac
phi ws create -t llm-app -n llm-app
```

```bash Windows
phi ws create -t llm-app -n llm-app
```

</CodeGroup>

This command creates a folder `llm-app` with a pre-built LLM App that you can customize and make your own. It has the following structure:

```bash
llm-app                       # root directory for your RAG llm-app
├── api                     # directory for FastApi routes
├── app                     # directory for Streamlit apps
├── db                      # directory for database components
├── llm                     # directory for LLM components
    ├── conversations       # LLM conversations
    ├── knowledge_base.py   # LLM knowledge base
    └── storage.py          # LLM storage
├── notebooks               # directory for Jupyter notebooks
├── Dockerfile              # Dockerfile for the application
├── pyproject.toml          # python project definition
├── requirements.txt        # python dependencies managed by pyproject.toml
├── scripts                 # directory for helper scripts
├── tests                   # directory for unit tests
├── utils                   # directory for shared utilities
└── workspace               # phidata workspace directory
    ├── dev_resources.py    # dev resources running locally
    ├── prd_resources.py    # production resources running on AWS
    ├── jupyter             # jupyter notebook resources
    ├── secrets             # directory for storing secrets
    └── settings.py         # phidata workspace settings
```

<Snippet file="set-openai-key.mdx" />

## Serve your App using Streamlit

[Streamlit](https://streamlit.io/) allows us to build
micro front-ends for our LLM App and is extremely useful for building basic applications
in pure python. Start the `app` group using:

<CodeGroup>

```bash terminal
phi ws up --group app
```

```bash shorthand
phi ws up dev:docker:app
```

</CodeGroup>

**Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.

### Chat with PDFs

- Open [localhost:8501](http://localhost:8501) to view streamlit apps that you can customize and make your own.
- Click on **Chat with PDFs** in the sidebar
- Enter a username and wait for the knowledge base to load.
- Choose the `RAG` Conversation type.
- Ask "How do I make chicken curry?"
- Upload PDFs and ask questions

![Chat with pdf](/images/rag-llm-app-chat-with-pdf.png)

Check the docker logs to see the "context" passed along with the question.

![RAG logs](/images/rag-llm-app-docker-logs.png)

## Add your data

The `pdf_knowledge_base` in the `llm/knowledge_base.py` file provides the references (context) used for RAG. To add your data:

<Steps>
  <Step title="Create a folder for your data">
    Create a folder `data/pdfs` in the root directory of your app
  </Step>
  <Step title="Add your data">Add your files to the `data/pdfs` folder</Step>
  <Step title="Update Knowledge Base">
    Click on the `Update Knowledge Base` button to load the knowledge base.
  </Step>
</Steps>

Checkout the `llm/knowledge_base.py` file for more information. The `PDFKnowledgeBase` reads local PDFs and the `PDFUrlKnowledgeBase` reads PDFs from URLs.

## How this App works

The streamlit apps are defined in the `app` folder and the `Conversations` powering these apps are defined in the `llm/conversations` folder.

Checkout the `llm/conversations/pdf_rag.py` file for more information:

- The `add_references_to_prompt` flag will populate the `references` argument of the `user_prompt_function`
- The `user_prompt_function` builds the **Prompt** sent to the LLM. The prompt is injected with additional information from the knowledge base using the `references` variable.

```python llm/conversations/pdf_rag.py
...
def get_pdf_rag_conversation(
    user_name: Optional[str] = None,
    conversation_id: Optional[str] = None,
    debug_mode: bool = False,
) -> Conversation:
    """Get a RAG conversation with the PDF knowledge base"""

    return Conversation(
        ...
        user_prompt_function=lambda message, references, **kwargs: f"""\
        Use the following information from the knowledge base if it helps.
        <knowledge_base>
        {references}
        </knowledge_base>

        Respond to the following message:
        USER: {message}
        ASSISTANT:
        """,
        # This populates the "references" argument to the user prompt function
        add_references_to_prompt=True,
        # This adds the last 8 messages to the API call
        add_chat_history_to_messages=True,
    )
```

## Serve your App using FastApi

Streamlit is great for building micro front-ends but any production application will be built using a front-end framework like [next.js](https://nextjs.org) backed by a RestApi built using [FastApi](https://fastapi.tiangolo.com/).

Your LLM App comes ready-to-use with FastApi endpoints, start the `api` group using:

<CodeGroup>

```bash terminal
phi ws up --group api
```

```bash shorthand
phi ws up dev:docker:api
```

</CodeGroup>

**Press Enter** to confirm

### View API Endpoints

- Open [localhost:8000/docs](http://localhost:8000/docs) to view the API Endpoints.
- Load the knowledge base using `/v1/pdf/conversation/load-knowledge-base`
- Test the `v1/pdf/conversation/chat` endpoint with

```json
{
  "message": "How do I make chicken curry?"
}
```

<img
  src="/images/rag-llm-app-fastapi-local.png"
  alt="rag-llm-app-fastapi-local"
/>

## Build your LLM Product

Your LLM Api provides common endpoints you can use to build your LLM product. These routes are developed in collaboration with real AI Apps and are a great starting point to build on.

For example:

- Call the `/conversation/create` endpoint to create a new conversation for a user.

```json
{
  "user_name": "my-app-user-1",
  "conversation_type": "RAG"
}
```

- The response contains a `conversation_id` that can be used to build a chat interface by calling the `/conversation/chat` endpoint.

```json
{
  "message": "how do I make pasta",
  "stream": true,
  "conversation_id": "fc67f202-c2cb-4c3d-a640-704ab83d9460",
  "conversation_type": "RAG"
}
```

These routes are defined the `api/routes` folder and can be customized to your use case.

<Tip>
  Message us on [discord](https://discord.gg/4MtYHHrgA8) if you need help.
</Tip>

## Optional: Run Jupyterlab

A jupyter notebook is a must have for AI development and your `llm-app` comes with a notebook pre-installed with the required dependencies. To start your notebook:

<Steps>
  <Step title="Enable Jupyter">
    Update the `workspace/settings.py` file and set `dev_jupyter_enabled=True`

    ```python workspace/settings.py
    ...
    ws_settings = WorkspaceSettings(
        ...
        # Uncomment the following line
        dev_jupyter_enabled=True,
    ...
    ```

  </Step>
  <Step title="Start Jupyter">

    <CodeGroup>

    ```bash terminal
    phi ws up --group jupyter
    ```

    ```bash shorthand
    phi ws up dev:docker:jupyter
    ```

    </CodeGroup>

    **Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.

  </Step>
  <Step title="View JupyterLab UI">
    - Open [localhost:8888](http://localhost:8888) to view the Jupyterlab UI. Password: **admin**
    - Play around with cookbooks in the `notebooks` folder.

    <img
      src="/images/rag-llm-app-jupyter-local.png"
      alt="rag-llm-app-jupyter-local"
    />

  </Step>
</Steps>

## Delete local resources

Play around and stop the workspace using:

<CodeGroup>

```bash terminal
phi ws down
```

```bash full options
phi ws down --env dev --infra docker
```

```bash shorthand
phi ws down dev:docker
```

</CodeGroup>

or stop individual Apps using:

<CodeGroup>

```bash app
phi ws down --group app
```

```bash api
phi ws down --group api
```

```bash database
phi ws down --group db
```

```bash jupyter
phi ws down --group jupyter
```

</CodeGroup>

## Next

Congratulations on running your LLM App locally. Next Steps:

- [Run your LLM App on AWS](/quickstart//run-aws)
- Create a [git repository for your workspace](/how-to/git-repo)
- Read how to [manage the development application](/how-to/development-app)
- Read how to [format and validate your code](/how-to/format-and-validate)
- Read how to [add python libraries](/how-to/python-libraries)
- Chat with us on [discord](https://discord.gg/4MtYHHrgA8)
