---
title: Groq
---

## Example

<CodeGroup>

```python agent.py
from phi.agent import Agent
from phi.llm.groq import Groq

agent = Agent(
    llm=Groq(model="mixtral-8x7b-32768"),
    description="You help people with their health and fitness goals.",
    # debug_mode=True,
)
agent.print_response("Share a quick healthy breakfast recipe.", markdown=True)
```

</CodeGroup>

## Groq Params

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `name` | `str` | `"Groq"` | Name of the Groq model |
| `model` | `str` | `"mixtral-8x7b-32768"` | The specific Groq model to use |
| `frequency_penalty` | `float` | - | Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. |
| `logit_bias` | `Any` | - | Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. |
| `logprobs` | `int` | - | - |
| `max_tokens` | `int` | - | The maximum number of tokens to generate in the chat completion. |
| `presence_penalty` | `float` | - | Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. |
| `response_format` | `Dict[str, Any]` | - | An object specifying the format that the model must output. Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON. |
| `seed` | `int` | - | If specified, the system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. |
| `stop` | `Union[str, List[str]]` | - | Up to 4 sequences where the API will stop generating further tokens. |
| `temperature` | `float` | - | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. |
| `top_logprobs` | `int` | - | - |
| `top_p` | `float` | - | An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. |
| `user` | `str` | - | A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. |
| `extra_headers` | `Any` | - | - |
| `extra_query` | `Any` | - | - |
| `api_key` | `str` | - | API key for Groq |
| `organization` | `str` | - | - |
| `base_url` | `str` | - | Base URL for the Groq API |
| `timeout` | `float` | - | - |
| `max_retries` | `int` | - | - |
| `default_headers` | `Any` | - | - |
| `default_query` | `Any` | - | - |
| `groq_client` | `GroqClient` | - | Custom Groq client, if provided |

## LLM Params

`Groq` is a subclass of the `LLM` class and has access to the same params

<Snippet file="llm-base-reference.mdx" />
