---
title: Ollama
---

## Example

<CodeGroup>

```python agent.py
from phi.agent import Agent
from phi.llm.ollama import Ollama

agent = Agent(
    llm=Ollama(),
    description="You help people with their health and fitness goals.",
)
agent.print_response("Share a quick healthy breakfast recipe.", markdown=True)
```

</CodeGroup>

## Ollama Params

| Parameter | Type | Default | Description |
| --- | --- | --- | --- |
| `name` | `str` | `"Ollama"` | Name for this LLM. Note: This is not sent to the LLM API. |
| `model` | `str` | `"llama2"` | ID of the model to use. |
| `host` | `str` | - | |
| `timeout` | `Any` | - | |
| `format` | `str` | - | The format to return a response in. Currently the only accepted value is `json` |
| `options` | `Any` | - | Additional model parameters such as temperature |
| `keep_alive` | `Union[float, str]` | - | Controls how long the model will stay loaded into memory following the request. |
| `function_call_limit` | `int` | `10` | Maximum number of function calls allowed across all iterations. |
| `deactivate_tools_after_use` | `bool` | `False` | Deactivate tool calls by turning off JSON mode after 1 tool call |
| `add_user_message_after_tool_call` | `bool` | `True` | After a tool call is run, add the user message as a reminder to the LLM |
| `ollama_client` | `OllamaClient` | - | |

## LLM Params

`Ollama` is a subclass of the `LLM` class and has access to the same params

<Snippet file="llm-base-reference.mdx" />
