---
title: LLMTask
sidebarTitle: LLM Task
---

## Example

```python story_generator.py
from phi.llm.openai import OpenAIChat
from phi.task.llm import LLMTask
from phi.assistant import Assistant
from pydantic import BaseModel, Field

class StoryTheme(BaseModel):
    setting: str = Field(..., description="This is the context of the story. If not available, provide a random setting.",)
    genre: str = Field(..., description="This is the genre of the story. If not provided, select horror.")

get_story_theme = LLMTask(
    system_prompt="Generate a theme for a story.",
    output_model=StoryTheme,
    show_output=False,
)

write_story = LLMTask(
    llm=OpenAIChat(model="gpt-4"),
    system_prompt="Write a 2 sentence story for a given theme. It should be less than 30 words.",
)

give_story_a_name = LLMTask(
    system_prompt="Give this story a 2 word name. Format output as `Name: {name}`. Don't surround with quotes.",
)

story_assistant = Assistant(tasks=[get_story_theme, write_story, give_story_a_name])
story_assistant.cli_app(user="Theme")
```

## LLMTask Reference

<ResponseField name="llm" type="LLM">
  LLM to use for this task
</ResponseField>
<ResponseField name="memory" type="LLMTaskMemory" default="LLMTaskMemory()">
  Task Memory
</ResponseField>
<ResponseField name="add_chat_history_to_messages" type="bool" default="False">
  Add chat history to the messages sent to the LLM.
  If True, the chat history is added to the messages sent to the LLM.
</ResponseField>
<ResponseField name="add_chat_history_to_prompt" type="bool" default="False">
  If True, adds the formatted chat history to the user prompt.
</ResponseField>
<ResponseField name="num_history_messages" type="int" default="8">
  Number of previous messages to add to prompt or messages sent to the LLM.
</ResponseField>
<ResponseField name="knowledge_base" type="AssistantKnowledge">
  Task Knowledge Base
</ResponseField>
<ResponseField name="add_references_to_prompt" type="bool" default="False">
  Enable RAG by adding references from the knowledge base to the prompt.
</ResponseField>

<ResponseField name="tools" type="List[Union[Tool, ToolRegistry, Callable, Dict, Function]]">
  A list of tools provided to the LLM.
  Tools are functions the model may generate JSON inputs for.
  If you provide a dict, it is not called by the model.
</ResponseField>
<ResponseField name="use_tools" type="bool" default="False">
 Allow the LLM to use tools
</ResponseField>
<ResponseField name="show_tool_calls" type="bool" default="False">
 Show tool calls in LLM messages.
</ResponseField>
<ResponseField name="tool_call_limit" type="int">
 Maximum number of tool calls allowed.
</ResponseField>
<ResponseField name="tool_choice" type="Union[str, Dict[str, Any]]">
 Controls which (if any) tool is called by the model.
 "none" means the model will not call a tool and instead generates a message.
 "auto" means the model can pick between generating a message or calling a tool.
 Specifying a particular function via `{"type: "function", "function": {"name": "my_function"}}`
 forces the model to call that tool.
 "none" is the default when no tools are present. "auto" is the default if tools are present.
</ResponseField>
<ResponseField name="update_knowledge_base" type="bool" default="False">
 If use_tools is True and update_knowledge_base is True,
 then a tool is added that allows the LLM to update the knowledge base.
</ResponseField>
<ResponseField name="read_tool_call_history" type="bool" default="False">
 If use_tools is True and read_tool_call_history is True,
 then a tool is added that allows the LLM to get the tool call history.
</ResponseField>

<ResponseField name="system_prompt" type="str">
  Provide the system prompt as a string
</ResponseField>
<ResponseField name="system_prompt_function" type="Callable[..., Optional[str]]">
  Provide the system prompt as a function. This function is provided the "Assistant object" as an argument
  and should return the system_prompt as a string.

Signature:

```python
def system_prompt_function(assistant: Assistant) -> str:
    ...
```

</ResponseField>
<ResponseField name="build_default_system_prompt" type="bool" default="True">
  If True, build a default system prompt using instructions and extra_instructions
</ResponseField>
<ResponseField name="description" type="str">
  Assistant description for the default system prompt
</ResponseField>
<ResponseField name="instructions" type="List[str]">
  List of instructions for the default system prompt
</ResponseField>
<ResponseField name="extra_instructions" type="List[str]">
  List of extra_instructions for the default system prompt
  Use these when you want to use the default prompt but also add some extra instructions
</ResponseField>
<ResponseField name="add_to_system_prompt" type="str">
  Add a string to the end of the default system prompt
</ResponseField>
<ResponseField name="add_knowledge_base_instructions" type="bool" default="True">
  If True, add instructions for using the knowledge base to the default system prompt if knowledge base is provided
</ResponseField>
<ResponseField name="prevent_hallucinations" type="bool" default="False">
  If True, add instructions for letting the user know that the assistant does not know the answer
</ResponseField>
<ResponseField name="prevent_prompt_injection" type="bool" default="False">
  If True, add instructions to prevent prompt injection attacks
</ResponseField>
<ResponseField name="limit_tool_access" type="bool" default="True">
  If True, add instructions to prevent prompt injection attacks
</ResponseField>
<ResponseField name="add_datetime_to_instructions" type="bool" default="False">
  If True, add the current datetime to the prompt to give the assistant a sense of time
  This allows for relative times like "tomorrow" to be used in the prompt
</ResponseField>
<ResponseField name="markdown" type="bool" default="False">
  If markdown=true, formats the output using markdown
</ResponseField>

<ResponseField name="user_prompt" type="Union[List[Dict], str]">
  Provides the user prompt as a string. Note: this will ignore the input message provided to the run function
</ResponseField>
<ResponseField name="user_prompt_function" type="Callable[..., str]">
  Provides the user prompt as a function. This function is provided the "Assistant object" and the "Input message" as arguments
  and should return the user_prompt as a Union[List[Dict], str].
  If `add_references_to_prompt` is True, then `references` are also provided as an argument.
  If `add_chat_history_to_prompt` is True, then `chat_history` is also provided as an argument.

Signature:

```python
def custom_user_prompt_function(
    assistant: Assistant,
    message: Union[List[Dict], str],
    references: Optional[str] = None,
    chat_history: Optional[str] = None,
) -> Union[List[Dict], str]:
    ...
```

</ResponseField>
<ResponseField name="build_default_user_prompt" type="bool" default="True">
  If True, build a default user prompt using references and chat history
</ResponseField>
<ResponseField name="references_function" type="Callable[..., Optional[str]]">
Function to build references for the default `user_prompt`. This function, if provided, is called when `add_references_to_prompt` is True

Signature:

```python
def references(assistant: Assistant, query: str) -> Optional[str]:
    ...
```

</ResponseField>
<ResponseField name="references_format" type="Literal['json', 'yaml']" default="json">
  Format of the references
</ResponseField>
<ResponseField name="chat_history_function" type="Callable[..., Optional[str]]">
Function to build the chat_history for the default `user_prompt`. This function, if provided, is called when `add_chat_history_to_prompt` is True

Signature:

```python
def chat_history(assistant: Assistant) -> str:
    ...
```

</ResponseField>

## Task Reference

`LLMTask` is a subclass of the `Task` class and has access to the same params

<Snippet file="task-base-reference.mdx" />